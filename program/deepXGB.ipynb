{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import asyncio ,nest_asyncio\n",
    "from python_tools.discord_bot import async_discord_bot_notifier ,discord_bot_notifier\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# model_select_signal = 'resnet18'\n",
    "model_select_signal = 'vgg16'\n",
    "# model_select_signal = 'densenet121'\n",
    "# model_select_signal = 'feature vec'\n",
    "\n",
    "\n",
    "data_select_signal = 'skin'\n",
    "# data_select_signal = 'chest CT'\n",
    "# data_select_signal = 'ocularDisease'\n",
    "\n",
    "# select image type is RGB or not\n",
    "isRGB = True\n",
    "# isRGB = False\n",
    "\n",
    "# enable img feature vector as mutimodal\n",
    "# enable_muti_modal_signal = True\n",
    "enable_muti_modal_signal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "from data.ocularDisease.ocularDataloader import OcularDiseaseDataProcessor\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'chest CT':\n",
    "    dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'ocularDisease':\n",
    "    dataContainer = OcularDiseaseDataProcessor(transform=transform_std)\n",
    "else:\n",
    "    raise ValueError('需要指定dataset類別')\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.getDataloaders()\n",
    "train_files , test_files = dataContainer.getDatasetFilenames()\n",
    "# feature_vector_file_path = dataContainer.getFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cal img feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "train_img_feature_vector = []\n",
    "test_img_feature_vector = []\n",
    "\n",
    "def print_list_dimensions(lst):\n",
    "    dimensions = []\n",
    "    while isinstance(lst, list):\n",
    "        dimensions.append(len(lst))\n",
    "        lst = lst[0] if len(lst) > 0 else []\n",
    "    print(\"Dimensions:\", \" x \".join(map(str, dimensions)))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if enable_muti_modal_signal:\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(train_dataloader):\n",
    "        for img in batch_data:\n",
    "            train_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(train_img_feature_vector)\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(test_dataloader):\n",
    "        for img in batch_data:\n",
    "            test_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(test_feature_vector)\n",
    "        \n",
    "    # for idx , (data, label) in enumerate(test_dataloader):\n",
    "    #     train_img_feature_vector.append( calImgFeatureVector(data, isRGB=isRGB) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB, calFPGAFormatXGB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 負責呼叫與紀錄結果\n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader,\n",
    "                    test_dataloader, enable_muti_module:bool=False,\n",
    "                    isRGB:bool=False, model_output_path:str=None,\n",
    "                    FPGA_format=False\n",
    "                    ):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  \n",
    "  start_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "  end_time = time.time()\n",
    "  CNN_eval_time = (end_time - start_time)/len(train_features)\n",
    "\n",
    "  # muti modal selection\n",
    "  if enable_muti_module:\n",
    "\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "\n",
    "  print(\"feature size is : {}\".format(test_features.shape))\n",
    "  # print_list_dimensions(test_features)\n",
    "  \n",
    "  # print(feature for feature in test_features if len(feature) == 0)\n",
    "  # nan_indices = np.where(np.isnan(test_features))\n",
    "  # print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "\n",
    "  \n",
    "  # xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  # 呼叫自定義的訓練func\n",
    "  if FPGA_format: # 呼叫符合FPGA中規定好的訓練\n",
    "    XGB_result = calFPGAFormatXGB(train_features, train_labels, test_features, test_labels, device, enable_f1_metric=True, model_output_path=model_output_path)\n",
    "  else : \n",
    "    XGB_result = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, device, enable_f1_metric=True, model_output_path=model_output_path)\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  # 計算CNN model的大小\n",
    "  total_size = 0\n",
    "  for p in model.parameters():\n",
    "    total_size += p.numel() * p.element_size()\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "  XGB_result['CNN size'] = total_size\n",
    "  XGB_result['CNN eval time'] = CNN_eval_time\n",
    "\n",
    "  return XGB_result\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, \n",
    "                   train_dataloader, test_dataloader,\n",
    "                   enable_muti_module=False):\n",
    "  \"\"\"使用剪枝後的CNN model 的輸出值來訓練ML model\"\"\"\n",
    "\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  str_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "\n",
    "  if enable_muti_module:\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "\n",
    "  \n",
    "  # print(len(test_features[0]))\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    model_folder_path = \"../model/HAM10000\"\n",
    "elif data_select_signal == 'chest CT':\n",
    "    model_folder_path = \"../model/CT chest\"\n",
    "elif data_select_signal == 'ocularDisease':\n",
    "    model_folder_path = \"../model/ocularDisease\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "# ================================\n",
    "# # 加載訓練好的densenet\n",
    "# densenet121 = models.densenet121(pretrained=True)\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.SGD(densenet121.parameters(), lr = 0.001) # 選擇你想用的 optimizer\n",
    "# # optimizer = torch.optim.Adam(model_densenet121.parameters(), lr =0.01)\n",
    "\n",
    "# # Loss function\n",
    "# loss_fn = nn.CrossEntropyLoss()                \n",
    "\n",
    "# # 更換classifier的輸出\n",
    "# densenet121.classifier = nn.Linear(densenet121.classifier.in_features, num_classes)\n",
    "\n",
    "\n",
    "# densenet121.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_densenet121.pth\"))\n",
    "# ================================\n",
    "# 設定空的model給feaure訓練用\n",
    "\n",
    "\n",
    "\n",
    "class EmptyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyModel, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回一个空的张量\n",
    "        return torch.empty((x.size(0), 0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "model_0 = None\n",
    "\n",
    "if model_select_signal == 'resnet18':\n",
    "    model_0 = resnet18\n",
    "elif model_select_signal == 'vgg16':\n",
    "    model_0 = vgg16\n",
    "elif model_select_signal == 'feature vec':\n",
    "    model_0 = nn.Sequential(EmptyModel())\n",
    "elif model_select_signal == 'densenet121':\n",
    "    model_0 = densenet121\n",
    "\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataContainer.dataset_label_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "\n",
    "# for idx , data in enumerate(train_features):\n",
    "#     train_features[idx] = np.concatenate(train_features[idx] , train_img_feature_vector[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "# print(len(train_features))\n",
    "# print(type(train_features))\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# total_layers = 0\n",
    "\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "\n",
    "#     total_layers += 1\n",
    "\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 44層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loss, acc, f1 score):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5040253672807936, 0.8299916457811195, 0.8196058895986511)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(\"(loss, acc, f1 score):\")\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 4096]                 --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 4096]                 --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 4096]                 --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "==========================================================================================\n",
       "Total params: 134,260,544\n",
       "Trainable params: 134,260,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.04\n",
       "Estimated Total Size (MB): 646.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[1][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_of_models[67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試每層所需要的時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @  calModelEvalTime\n",
    "\n",
    "# from helperFunction.helperFunctions import calModelEvalTime\n",
    "\n",
    "# # eval_time_res = calModelEvalTime([(model_0, \"layer0\")], device)\n",
    "# eval_time_res = calModelEvalTime(list_of_models, device)\n",
    "\n",
    "# # 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "# df_transposed = pd.DataFrame.from_dict(eval_time_res, orient='index').transpose()\n",
    "\n",
    "# # 將轉置後的DataFrame存儲為CSV檔案\n",
    "# csv_file_path_transposed = \"eval_time_result_\" + model_0_name + \"_\"+ device +  \".csv\"\n",
    "# df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# asyncio.run(async_discord_bot_notifier(\"model eval time 計算完成!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試每層所佔的容量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name : layer:44\n",
      "Model 1 Size: 512.27 MB\n",
      "model name : layer:43\n",
      "Model 2 Size: 512.16 MB\n",
      "model name : layer:42\n",
      "Model 3 Size: 512.16 MB\n",
      "model name : layer:41\n",
      "Model 4 Size: 512.16 MB\n",
      "model name : layer:40\n",
      "Model 5 Size: 448.15 MB\n",
      "model name : layer:39\n",
      "Model 6 Size: 448.15 MB\n",
      "model name : layer:38\n",
      "Model 7 Size: 448.15 MB\n",
      "model name : layer:37\n",
      "Model 8 Size: 56.13 MB\n",
      "model name : layer:36\n",
      "Model 9 Size: 56.13 MB\n",
      "model name : layer:35\n",
      "Model 10 Size: 56.13 MB\n",
      "model name : layer:34\n",
      "Model 11 Size: 56.13 MB\n",
      "model name : layer:33\n",
      "Model 12 Size: 56.13 MB\n",
      "model name : layer:32\n",
      "Model 13 Size: 56.13 MB\n",
      "model name : layer:31\n",
      "Model 14 Size: 56.13 MB\n",
      "model name : layer:30\n",
      "Model 15 Size: 47.13 MB\n",
      "model name : layer:29\n",
      "Model 16 Size: 47.13 MB\n",
      "model name : layer:28\n",
      "Model 17 Size: 38.13 MB\n",
      "model name : layer:27\n",
      "Model 18 Size: 38.13 MB\n",
      "model name : layer:26\n",
      "Model 19 Size: 29.13 MB\n",
      "model name : layer:25\n",
      "Model 20 Size: 29.13 MB\n",
      "model name : layer:24\n",
      "Model 21 Size: 29.13 MB\n",
      "model name : layer:23\n",
      "Model 22 Size: 20.12 MB\n",
      "model name : layer:22\n",
      "Model 23 Size: 20.12 MB\n",
      "model name : layer:21\n",
      "Model 24 Size: 11.12 MB\n",
      "model name : layer:20\n",
      "Model 25 Size: 11.12 MB\n",
      "model name : layer:19\n",
      "Model 26 Size: 6.62 MB\n",
      "model name : layer:18\n",
      "Model 27 Size: 6.62 MB\n",
      "model name : layer:17\n",
      "Model 28 Size: 6.62 MB\n",
      "model name : layer:16\n",
      "Model 29 Size: 4.37 MB\n",
      "model name : layer:15\n",
      "Model 30 Size: 4.37 MB\n",
      "model name : layer:14\n",
      "Model 31 Size: 2.12 MB\n",
      "model name : layer:13\n",
      "Model 32 Size: 2.12 MB\n",
      "model name : layer:12\n",
      "Model 33 Size: 0.99 MB\n",
      "model name : layer:11\n",
      "Model 34 Size: 0.99 MB\n",
      "model name : layer:10\n",
      "Model 35 Size: 0.99 MB\n",
      "model name : layer:9\n",
      "Model 36 Size: 0.43 MB\n",
      "model name : layer:8\n",
      "Model 37 Size: 0.43 MB\n",
      "model name : layer:7\n",
      "Model 38 Size: 0.15 MB\n",
      "model name : layer:6\n",
      "Model 39 Size: 0.15 MB\n",
      "model name : layer:5\n",
      "Model 40 Size: 0.15 MB\n",
      "model name : layer:4\n",
      "Model 41 Size: 0.01 MB\n",
      "model name : layer:3\n",
      "Model 42 Size: 0.01 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义函数计算模型参数大小\n",
    "def calculate_model_size(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    bytes_per_param = next(model.parameters()).element_size()\n",
    "    total_size_bytes = total_params * bytes_per_param\n",
    "    total_size_megabytes = total_size_bytes / (1024 ** 2)  # 转换为兆字节\n",
    "    return total_size_megabytes\n",
    "\n",
    "\n",
    "\n",
    "# 计算并打印每个模型的内存占用\n",
    "for idx, model in enumerate(list_of_models):\n",
    "    try:\n",
    "        model_size = calculate_model_size(model[0])\n",
    "    except:\n",
    "        break\n",
    "    print(f\"model name : {model[1]}\")\n",
    "    print(f\"Model {idx+1} Size: {model_size:.2f} MB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @ test unit : check if muti modal function is ok\n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "\n",
    "\n",
    "# test_muti_modal_vectors = []\n",
    "# for idx , data in enumerate(test_features):\n",
    "#     test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "# test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "# nan_indices = np.where(np.isnan(test_features))\n",
    "# print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "# print(test_features[1])\n",
    "# print(test_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list, test_list = dataContainer.getFilenamesList()\n",
    "\n",
    "# len(train_list), len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<contextlib._GeneratorContextManager object at 0x000001C9709B6490>\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.config_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/HAM10000/XGB/skin_vgg16_layer44\n",
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "label size is :2003\n",
      "feature size is :8012\n",
      "label size is :8012\n",
      "feature size is : (2003, 7)\n",
      "[01:56:23] INFO: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\simple_dmatrix.cc:137: Generating new Ellpack page.\n",
      "res's len : 2003\n",
      "feature's len : 2003\n",
      "跑測試資料的時間:8.406842225549732e-08\n",
      "['f1', 'f4', 'f6', 'f0', 'f3', 'f5', 'f2']\n",
      "[('f1', 789.0), ('f4', 740.0), ('f6', 723.0), ('f0', 693.0), ('f3', 672.0), ('f5', 623.0), ('f2', 566.0)]\n",
      "[01:56:26] INFO: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\simple_dmatrix.cc:137: Generating new Ellpack page.\n",
      "======eval finish!=========\n",
      "best acc is : -0.817773\n",
      "best f1 is : -0.811464\n",
      "XGB eval time : 0.0001683890497777611\n",
      "../model/HAM10000/XGB/skin_vgg16_layer43\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# training XGB !!!\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     XGB_result \u001b[38;5;241m=\u001b[39m \u001b[43mrecordXGBoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFPGA_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_FPGA_format_XGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough memory!! escape form loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mrecordXGBoutput\u001b[1;34m(model, train_dataloader, test_dataloader, enable_muti_module, isRGB, model_output_path, FPGA_format)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecordXGBoutput\u001b[39m(model:nn\u001b[38;5;241m.\u001b[39mSequential, train_dataloader,\n\u001b[0;32m      9\u001b[0m                     test_dataloader, enable_muti_module:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     10\u001b[0m                     isRGB:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, model_output_path:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m                     FPGA_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     12\u001b[0m                     ):\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcal CNN model output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m   test_features , test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdataloaderToFeatureData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m   train_features , train_labels \u001b[38;5;241m=\u001b[39m dataloaderToFeatureData(model, train_dataloader, device)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\helperFunctions.py:52\u001b[0m, in \u001b[0;36mdataloaderToFeatureData\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     51\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m---> 52\u001b[0m output ,label \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m , label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m     53\u001b[0m output \u001b[38;5;241m=\u001b[39m flattenExceptDim0(output)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# print(output.shape)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "enable_FPGA_format_XGB = True\n",
    "# enable_FPGA_format_XGB = False\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[],\n",
    "       \"CNN eval time\":[],\n",
    "       \"XGB eval time\":[],\n",
    "       \"CNN size\":[],\n",
    "       \"XGB size\":[],\n",
    "       }\n",
    "\n",
    "if enable_FPGA_format_XGB:\n",
    "    XGB_res[\"f_acc\"] = []\n",
    "    XGB_res[\"f_f1\"] = []\n",
    "    XGB_res[\"f_iter\"] = []\n",
    "    XGB_res[\"f_XGB_size\"] = []\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "# list_of_models = list_of_models[0:2]\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    model = model.to(device)\n",
    "    model_name = model_name.replace(\":\",\"\")\n",
    "    xgb_model_folder = model_folder_path + \"/XGB/\"\n",
    "    xgb_output_model_name = data_select_signal + \"_\" + model_select_signal + \"_\"  +  model_name \n",
    "    xgb_model_path =  xgb_model_folder + xgb_output_model_name\n",
    "    \n",
    "    print(xgb_model_path)\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        except MemoryError:\n",
    "            print(\"Not enough memory!! escape form loop\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"error catch :{}\".format(e))\n",
    "            break\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    # training XGB !!!\n",
    "    try:\n",
    "        XGB_result = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal, model_output_path=xgb_model_path, FPGA_format=enable_FPGA_format_XGB)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    # except:\n",
    "    #     print(\"error catch \")\n",
    "    #     break\n",
    "\n",
    "    iter = XGB_result['iter']\n",
    "\n",
    "    print(\"best acc is : {}\".format(XGB_result['acc'][iter]))\n",
    "    print(\"best f1 is : {}\".format(XGB_result['f1'][iter]))\n",
    "    print(\"XGB eval time : {}\".format(XGB_result['XGB eval time']) )\n",
    "\n",
    "    # print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "\n",
    "    cur_acc = XGB_result['acc'][iter]\n",
    "    XGB_res[\"acc\"].append((-cur_acc) if cur_acc < 0 else cur_acc)\n",
    "    cur_f1 = XGB_result['f1'][iter]\n",
    "    XGB_res[\"f1\"].append((-cur_f1) if cur_f1 < 0 else cur_f1)\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "\n",
    "    XGB_res[\"CNN eval time\"].append(XGB_result['CNN eval time'])\n",
    "    XGB_res[\"XGB eval time\"].append(XGB_result['XGB eval time'])\n",
    "    XGB_res[\"CNN size\"].append(XGB_result['CNN size'])\n",
    "    XGB_res[\"XGB size\"].append(XGB_result['XGB size'])\n",
    "\n",
    "    if enable_FPGA_format_XGB:\n",
    "        f_iter = XGB_result['f_iter']\n",
    "        XGB_res[\"f_iter\"].append(f_iter)\n",
    "        XGB_res[\"f_acc\"].append(XGB_result['f_acc'][f_iter])\n",
    "        XGB_res[\"f_f1\"].append(XGB_result['f_f1'][f_iter])\n",
    "        XGB_res[\"f_XGB_size\"].append(XGB_result['f_XGB_size'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(\n",
    "    async_discord_bot_notifier(\"XGB訓練完成!\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput size is :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_size))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mrecordMLoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#     print(\"ERROR!!!\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# cal output size\u001b[39;00m\n\u001b[0;32m     49\u001b[0m linear_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mrecordMLoutput\u001b[1;34m(ML_model, model, train_dataloader, test_dataloader, enable_muti_module)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"使用剪枝後的CNN model 的輸出值來訓練ML model\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcal CNN model output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m test_features , test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdataloaderToFeatureData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m str_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m      9\u001b[0m train_features , train_labels \u001b[38;5;241m=\u001b[39m dataloaderToFeatureData(model, train_dataloader, device)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\helperFunctions.py:48\u001b[0m, in \u001b[0;36mdataloaderToFeatureData\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 48\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\CustomImageDataset.py:34\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_to_label_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx]]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 34\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2174\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2166\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2167\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2168\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2169\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2170\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2171\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2172\u001b[0m         )\n\u001b[1;32m-> 2174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB105\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"ERROR!!!\")\n",
    "            break\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # try:\n",
    "        f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "        # except:\n",
    "        #     print(\"ERROR!!!\")\n",
    "        #     break\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    \n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" +  model_select_signal + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(async_discord_bot_notifier(\"ML model訓練完成!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
