{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import asyncio ,nest_asyncio\n",
    "from python_tools.discord_bot import async_discord_bot_notifier ,discord_bot_notifier\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_select_signal = 'resnet18'\n",
    "# model_select_signal = 'vgg16'\n",
    "\n",
    "# data_select_signal = 'skin'\n",
    "data_select_signal = 'chest CT'\n",
    "\n",
    "# select image type is RGB or not\n",
    "isRGB = True\n",
    "# isRGB = False\n",
    "\n",
    "# enable img feature vector as mutimodal\n",
    "enable_muti_modal_signal = True\n",
    "# enable_muti_modal_signal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['adenocarcinoma', 'large.cell.carcinoma', 'normal', 'squamous.cell.carcinoma']\n",
      "Class to index: {'adenocarcinoma': 0, 'large.cell.carcinoma': 1, 'normal': 2, 'squamous.cell.carcinoma': 3}\n",
      "Samples: [('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000108 (3).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (2).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (4).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (5).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000112 (2).png', 0)]\n",
      "First image size: torch.Size([3, 224, 224])\n",
      "First image label: 0\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'chest CT':\n",
    "    dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "else:\n",
    "    raise ValueError('需要指定dataset類別')\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.returnDataloaders()\n",
    "train_files , test_files = dataContainer.returnDatasetFilenames()\n",
    "feature_vector_file_path = dataContainer.returnFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cal img feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\feature\\texture.py:353: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "train_img_feature_vector = []\n",
    "test_img_feature_vector = []\n",
    "\n",
    "def print_list_dimensions(lst):\n",
    "    dimensions = []\n",
    "    while isinstance(lst, list):\n",
    "        dimensions.append(len(lst))\n",
    "        lst = lst[0] if len(lst) > 0 else []\n",
    "    print(\"Dimensions:\", \" x \".join(map(str, dimensions)))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if enable_muti_modal_signal:\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(train_dataloader):\n",
    "        for img in batch_data:\n",
    "            train_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(train_img_feature_vector)\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(test_dataloader):\n",
    "        for img in batch_data:\n",
    "            test_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(test_feature_vector)\n",
    "        \n",
    "    # for idx , (data, label) in enumerate(test_dataloader):\n",
    "    #     train_img_feature_vector.append( calImgFeatureVector(data, isRGB=isRGB) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module:bool=False, isRGB:bool=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  \n",
    "  start_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "  end_time = time.time()\n",
    "  CNN_eval_time = (end_time - start_time)/len(train_features)\n",
    "\n",
    "  # muti modal selection\n",
    "  if enable_muti_module:\n",
    "\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "\n",
    "  print(\"feature size is : {}\".format(test_features.shape))\n",
    "  # print_list_dimensions(test_features)\n",
    "  \n",
    "  # print(feature for feature in test_features if len(feature) == 0)\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "  # print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "\n",
    "  \n",
    "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  iter ,f1 ,acc, XGB_eval_time = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, enable_f1_metric=True)\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc, iter, CNN_eval_time, XGB_eval_time\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  str_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "\n",
    "  if enable_muti_module:\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "\n",
    "  \n",
    "  # print(len(test_features[0]))\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 4]                    --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 4]                    --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 4]                    --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-7                  [1, 4]                    16,388\n",
       "==========================================================================================\n",
       "Total params: 134,276,932\n",
       "Trainable params: 134,276,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.11\n",
       "Estimated Total Size (MB): 646.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    model_folder_path = \"../model/HAM10000\"\n",
    "elif data_select_signal == 'chest CT':\n",
    "    model_folder_path = \"../model/CT chest\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "summary(vgg16, [1,3,224,224])\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "model_0 = None\n",
    "\n",
    "if model_select_signal == 'resnet18':\n",
    "    model_0 = resnet18\n",
    "elif model_select_signal == 'vgg16':\n",
    "    model_0 = vgg16\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "\n",
    "# for idx , data in enumerate(train_features):\n",
    "#     train_features[idx] = np.concatenate(train_features[idx] , train_img_feature_vector[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "# print(len(train_features))\n",
    "# print(type(train_features))\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 初始化层计数器\n",
    "# total_layers = 0\n",
    "\n",
    "# # 遍历模型的所有子模块和层\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # 打印每一层的名称和它的具体类型，这一步是可选的，但对理解模型结构很有帮助\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "#     # 对所有层进行计数（包括卷积层、全连接层等）\n",
    "#     # 如果只想计算特定类型的层（如卷积层Conv2d），则需要添加判断条件\n",
    "#     total_layers += 1\n",
    "\n",
    "# # 打印总层数\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# # 示例：仅计算Conv2d层的数量\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 68層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5208647191524506, 0.7846064814814815, 0.7836901060941615)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.helperFunctions import calDetailModelLayersNum\n",
    "\n",
    "\n",
    "# summary(list_of_models[23][0], input_size=[1,3,224,224],depth=4)\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0)):\n",
    "#     try : \n",
    "#         summary(list_of_models[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 512, 7, 7]            --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "==========================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 85.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[2][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :2003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "\n",
    "test_muti_modal_vectors = []\n",
    "for idx , data in enumerate(test_features):\n",
    "    test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "test_features = np.array(test_muti_modal_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of NaN values: []\n",
      "[ 5.48047781  0.16579564 -1.28374267 -1.37016511  0.39112768 -0.97877556\n",
      " -0.61220598 -1.57313181 -1.589117    0.63244976  0.63244976  0.63244976\n",
      "  0.63244976  0.63244976  0.05713887  0.07180724  0.08428332  0.14377392\n",
      "  0.17865115  0.13785475  0.0850008   0.06814015  0.06154337  0.11180644\n",
      "  0.96723235  0.03873184  0.04978326  0.24593291  0.          0.\n",
      "  0.          0.          0.92066318  0.3883338   0.0397016   0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "[ 0.23694101  1.85144663 -0.08630075 -1.25937736 -0.99732959  1.77859294\n",
      "  0.06864095 -0.07236832  0.54394361  0.66211732  0.51233733 -1.67495278\n",
      " -1.24889012  1.27781296  0.05883291  0.06812022  0.08902663  0.14245855\n",
      "  0.17151626  0.14090402  0.09044165  0.06666534  0.0598294   0.11220504\n",
      "  0.30736113  0.21707693  0.88701373  0.26760662  0.          0.\n",
      "  0.          0.          0.77097738  0.62835687  0.1037376   0.\n",
      "  0.          0.          0.          0.          1.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "nan_indices = np.where(np.isnan(test_features))\n",
    "print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "print(test_features[1])\n",
    "print(test_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 48)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D380>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 5\n",
      "======eval finish!=========\n",
      "best acc is : -0.834249\n",
      "best f1 is : -0.832953\n",
      "XGB eval time : 6.23929399403702e-07\n",
      "model_name : layer:68\n",
      "input size is :[1, 512, 1, 1]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 553)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D000>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 103\n",
      "======eval finish!=========\n",
      "best acc is : -0.836745\n",
      "best f1 is : -0.834307\n",
      "XGB eval time : 7.363717911901679e-06\n",
      "model_name : layer:67\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D000>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 52\n",
      "======eval finish!=========\n",
      "best acc is : -0.830255\n",
      "best f1 is : -0.826461\n",
      "XGB eval time : 0.0001150771670500993\n",
      "model_name : layer:66\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D000>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 55\n",
      "======eval finish!=========\n",
      "best acc is : -0.834249\n",
      "best f1 is : -0.830722\n",
      "XGB eval time : 0.00010247841752652699\n",
      "model_name : layer:65\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 59\n",
      "======eval finish!=========\n",
      "best acc is : -0.823265\n",
      "best f1 is : -0.81678\n",
      "XGB eval time : 5.10824601530017e-05\n",
      "model_name : layer:64\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 95\n",
      "======eval finish!=========\n",
      "best acc is : -0.823764\n",
      "best f1 is : -0.818655\n",
      "XGB eval time : 0.00010309496701983767\n",
      "model_name : layer:63\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 70\n",
      "======eval finish!=========\n",
      "best acc is : -0.826261\n",
      "best f1 is : -0.820326\n",
      "XGB eval time : 9.650560360935648e-05\n",
      "model_name : layer:62\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 104\n",
      "======eval finish!=========\n",
      "best acc is : -0.814279\n",
      "best f1 is : -0.801448\n",
      "XGB eval time : 8.424546679317267e-05\n",
      "model_name : layer:61\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 97\n",
      "======eval finish!=========\n",
      "best acc is : -0.814279\n",
      "best f1 is : -0.80302\n",
      "XGB eval time : 0.00014440827648221882\n",
      "model_name : layer:60\n",
      "mat mismatch!!\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309C580>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 69\n",
      "======eval finish!=========\n",
      "best acc is : -0.807289\n",
      "best f1 is : -0.795221\n",
      "XGB eval time : 0.00016897994163806476\n",
      "model_name : layer:58\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D000>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 94\n",
      "======eval finish!=========\n",
      "best acc is : -0.810285\n",
      "best f1 is : -0.798225\n",
      "XGB eval time : 0.00011120536478531105\n",
      "model_name : layer:57\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 25129)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x0000017CE309D000>\n",
      "Indices of NaN values: []\n",
      "Best iteration: 99\n",
      "======eval finish!=========\n",
      "best acc is : -0.808288\n",
      "best f1 is : -0.796047\n",
      "XGB eval time : 0.00019504288346304397\n",
      "model_name : layer:56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-16 15:22:55] [INFO    ] discord.client: logging in using static token\n",
      "[2024-03-16 15:22:56] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: e8d3db4c398a5f3be29708c56d8f136e).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已登录为 python bot#2920\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[],\n",
    "       \"CNN eval time\":[],\n",
    "       \"XGB eval time\":[],\n",
    "       }\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        except MemoryError:\n",
    "            print(\"Not enough memory!! escape form loop\")\n",
    "            break\n",
    "        except:\n",
    "            print(\"error catch \")\n",
    "            break\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    # training XGB !!\n",
    "    try:\n",
    "        f1, acc, iter, CNN_eval_time, XGB_eval_time = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    # except:\n",
    "    #     print(\"error catch \")\n",
    "    #     break\n",
    "\n",
    "    print(\"best acc is : {}\".format(acc[iter]))\n",
    "    print(\"best f1 is : {}\".format(f1[iter]))\n",
    "    print(\"XGB eval time : {}\".format(XGB_eval_time) )\n",
    "\n",
    "    print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    cur_acc = acc[iter]\n",
    "    XGB_res[\"acc\"].append((-cur_acc) if cur_acc < 0 else cur_acc)\n",
    "    cur_f1 = f1[iter]\n",
    "    XGB_res[\"f1\"].append((-cur_f1) if cur_f1 < 0 else cur_f1)\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "    XGB_res[\"CNN eval time\"].append(CNN_eval_time)\n",
    "    XGB_res[\"XGB eval time\"].append(XGB_eval_time)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 12:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(\n",
    "    async_discord_bot_notifier(\"XGB訓練完成!\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 7.8245 秒\n",
      "預測時間 in 0.0989 秒\n",
      "預測時間 in 0.0275 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8262 , 0.8318。\n",
      "======eval finish!=========\n",
      "layer:68\n",
      "input size is :[1, 512, 1, 1]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 62.4685 秒\n",
      "預測時間 in 0.1930 秒\n",
      "預測時間 in 0.0460 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8262 , 0.8318。\n",
      "======eval finish!=========\n",
      "layer:67\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 316.6516 秒\n",
      "預測時間 in 0.8759 秒\n",
      "預測時間 in 0.2350 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8176 , 0.8238。\n",
      "======eval finish!=========\n",
      "layer:66\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 557.6577 秒\n",
      "預測時間 in 0.8741 秒\n",
      "預測時間 in 0.2240 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8201 , 0.8273。\n",
      "======eval finish!=========\n",
      "layer:65\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 144.9218 秒\n",
      "預測時間 in 0.9413 秒\n",
      "預測時間 in 0.2430 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7839 , 0.7998。\n",
      "======eval finish!=========\n",
      "layer:64\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 662.1575 秒\n",
      "預測時間 in 0.9230 秒\n",
      "預測時間 in 0.2380 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7837 , 0.7993。\n",
      "======eval finish!=========\n",
      "layer:63\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 652.1968 秒\n",
      "預測時間 in 0.9222 秒\n",
      "預測時間 in 0.2573 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7903 , 0.8053。\n",
      "======eval finish!=========\n",
      "layer:62\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 151.4665 秒\n",
      "預測時間 in 0.9240 秒\n",
      "預測時間 in 0.2391 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7173 , 0.7619。\n",
      "======eval finish!=========\n",
      "layer:61\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 149.8844 秒\n",
      "預測時間 in 0.9073 秒\n",
      "預測時間 in 0.2490 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7239 , 0.7654。\n",
      "======eval finish!=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-16 16:22:22] [INFO    ] discord.client: logging in using static token\n",
      "[2024-03-16 16:22:22] [INFO    ] discord.client: logging in using static token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:60\n",
      "ERROR!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-03-16 16:22:23] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: cc30ab71659eaee3dc8dcd2a820d86bc).\n",
      "[2024-03-16 16:22:23] [INFO    ] discord.gateway: Shard ID None has connected to Gateway (Session ID: cc30ab71659eaee3dc8dcd2a820d86bc).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已登录为 python bot#2920\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "An asyncio.Future, a coroutine or an awaitable is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 74\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# 返回CSV檔案的儲存路徑\u001b[39;00m\n\u001b[0;32m     72\u001b[0m csv_file_path_transposed\n\u001b[1;32m---> 74\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscord_bot_notifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mML model訓練完成!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\nest_asyncio.py:29\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     27\u001b[0m loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m     28\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_debug(debug)\n\u001b[1;32m---> 29\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:649\u001b[0m, in \u001b[0;36mensure_future\u001b[1;34m(coro_or_future, loop)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mensure_future\u001b[39m(coro_or_future, \u001b[38;5;241m*\u001b[39m, loop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    645\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrap a coroutine or an awaitable in a future.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \n\u001b[0;32m    647\u001b[0m \u001b[38;5;124;03m    If the argument is a Future, it is returned directly.\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ensure_future\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro_or_future\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py:664\u001b[0m, in \u001b[0;36m_ensure_future\u001b[1;34m(coro_or_future, loop)\u001b[0m\n\u001b[0;32m    662\u001b[0m         called_wrap_awaitable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn asyncio.Future, a coroutine or an awaitable \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    665\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis required\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    668\u001b[0m     loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39m_get_event_loop(stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: An asyncio.Future, a coroutine or an awaitable is required"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB105\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"ERROR!!!\")\n",
    "            break\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # try:\n",
    "        f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "        # except:\n",
    "        #     print(\"ERROR!!!\")\n",
    "        #     break\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    \n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" +  model_select_signal + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(async_discord_bot_notifier(\"ML model訓練完成!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
