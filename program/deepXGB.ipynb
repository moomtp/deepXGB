{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import asyncio ,nest_asyncio\n",
    "from python_tools.discord_bot import async_discord_bot_notifier ,discord_bot_notifier\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "# model_select_signal = 'resnet18'\n",
    "model_select_signal = 'vgg16'\n",
    "# model_select_signal = 'densenet121'\n",
    "# model_select_signal = 'feature vec'\n",
    "\n",
    "\n",
    "data_select_signal = 'skin'\n",
    "# data_select_signal = 'chest CT'\n",
    "# data_select_signal = 'ocularDisease'\n",
    "\n",
    "# select image type is RGB or not\n",
    "isRGB = True\n",
    "# isRGB = False\n",
    "\n",
    "# enable img feature vector as mutimodal\n",
    "# enable_muti_modal_signal = True\n",
    "enable_muti_modal_signal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "from data.ocularDisease.ocularDataloader import OcularDiseaseDataProcessor\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'chest CT':\n",
    "    dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'ocularDisease':\n",
    "    dataContainer = OcularDiseaseDataProcessor(transform=transform_std)\n",
    "else:\n",
    "    raise ValueError('需要指定dataset類別')\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.getDataloaders()\n",
    "train_files , test_files = dataContainer.getDatasetFilenames()\n",
    "# feature_vector_file_path = dataContainer.getFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cal img feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "train_img_feature_vector = []\n",
    "test_img_feature_vector = []\n",
    "\n",
    "def print_list_dimensions(lst):\n",
    "    dimensions = []\n",
    "    while isinstance(lst, list):\n",
    "        dimensions.append(len(lst))\n",
    "        lst = lst[0] if len(lst) > 0 else []\n",
    "    print(\"Dimensions:\", \" x \".join(map(str, dimensions)))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if enable_muti_modal_signal:\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(train_dataloader):\n",
    "        for img in batch_data:\n",
    "            train_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(train_img_feature_vector)\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(test_dataloader):\n",
    "        for img in batch_data:\n",
    "            test_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(test_feature_vector)\n",
    "        \n",
    "    # for idx , (data, label) in enumerate(test_dataloader):\n",
    "    #     train_img_feature_vector.append( calImgFeatureVector(data, isRGB=isRGB) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 負責呼叫與紀錄結果\n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module:bool=False, isRGB:bool=False, model_output_path:str=None):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  \n",
    "  start_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "  end_time = time.time()\n",
    "  CNN_eval_time = (end_time - start_time)/len(train_features)\n",
    "\n",
    "  # muti modal selection\n",
    "  if enable_muti_module:\n",
    "\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "\n",
    "  print(\"feature size is : {}\".format(test_features.shape))\n",
    "  # print_list_dimensions(test_features)\n",
    "  \n",
    "  # print(feature for feature in test_features if len(feature) == 0)\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "  # print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "\n",
    "  \n",
    "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  # 呼叫自定義的訓練func\n",
    "  iter ,f1 ,acc, XGB_eval_time = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, device, enable_f1_metric=True, model_output_path=model_output_path)\n",
    "  # TODO : 呼叫符合FPGA中規定好的訓練\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc, iter, CNN_eval_time, XGB_eval_time\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  str_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "\n",
    "  if enable_muti_module:\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "\n",
    "  \n",
    "  # print(len(test_features[0]))\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    model_folder_path = \"../model/HAM10000\"\n",
    "elif data_select_signal == 'chest CT':\n",
    "    model_folder_path = \"../model/CT chest\"\n",
    "elif data_select_signal == 'ocularDisease':\n",
    "    model_folder_path = \"../model/ocularDisease\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "# ================================\n",
    "# # 加載訓練好的densenet\n",
    "# densenet121 = models.densenet121(pretrained=True)\n",
    "# # Optimizer\n",
    "# optimizer = torch.optim.SGD(densenet121.parameters(), lr = 0.001) # 選擇你想用的 optimizer\n",
    "# # optimizer = torch.optim.Adam(model_densenet121.parameters(), lr =0.01)\n",
    "\n",
    "# # Loss function\n",
    "# loss_fn = nn.CrossEntropyLoss()                \n",
    "\n",
    "# # 更換classifier的輸出\n",
    "# densenet121.classifier = nn.Linear(densenet121.classifier.in_features, num_classes)\n",
    "\n",
    "\n",
    "# densenet121.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_densenet121.pth\"))\n",
    "# ================================\n",
    "# 設定空的model給feaure訓練用\n",
    "\n",
    "\n",
    "\n",
    "class EmptyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmptyModel, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回一个空的张量\n",
    "        return torch.empty((x.size(0), 0), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "model_0 = None\n",
    "\n",
    "if model_select_signal == 'resnet18':\n",
    "    model_0 = resnet18\n",
    "elif model_select_signal == 'vgg16':\n",
    "    model_0 = vgg16\n",
    "elif model_select_signal == 'feature vec':\n",
    "    model_0 = nn.Sequential(EmptyModel())\n",
    "elif model_select_signal == 'densenet121':\n",
    "    model_0 = densenet121\n",
    "\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataContainer.dataset_label_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "\n",
    "# for idx , data in enumerate(train_features):\n",
    "#     train_features[idx] = np.concatenate(train_features[idx] , train_img_feature_vector[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "# print(len(train_features))\n",
    "# print(type(train_features))\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 初始化层计数器\n",
    "# total_layers = 0\n",
    "\n",
    "# # 遍历模型的所有子模块和层\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # 打印每一层的名称和它的具体类型，这一步是可选的，但对理解模型结构很有帮助\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "#     # 对所有层进行计数（包括卷积层、全连接层等）\n",
    "#     # 如果只想计算特定类型的层（如卷积层Conv2d），则需要添加判断条件\n",
    "#     total_layers += 1\n",
    "\n",
    "# # 打印总层数\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# # 示例：仅计算Conv2d层的数量\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 44層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(loss, acc, f1 score):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5040253672807936, 0.8299916457811195, 0.8196058895986511)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(\"(loss, acc, f1 score):\")\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 4096]                 --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 4096]                 --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 4096]                 --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "==========================================================================================\n",
       "Total params: 134,260,544\n",
       "Trainable params: 134,260,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.04\n",
       "Estimated Total Size (MB): 646.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[1][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list_of_models[67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 測試每層所需要的時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @  calModelEvalTime\n",
    "\n",
    "# from helperFunction.helperFunctions import calModelEvalTime\n",
    "\n",
    "# # eval_time_res = calModelEvalTime([(model_0, \"layer0\")], device)\n",
    "# eval_time_res = calModelEvalTime(list_of_models, device)\n",
    "\n",
    "# # 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "# df_transposed = pd.DataFrame.from_dict(eval_time_res, orient='index').transpose()\n",
    "\n",
    "# # 將轉置後的DataFrame存儲為CSV檔案\n",
    "# csv_file_path_transposed = \"eval_time_result_\" + model_0_name + \"_\"+ device +  \".csv\"\n",
    "# df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# asyncio.run(async_discord_bot_notifier(\"model eval time 計算完成!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # @ test unit : check if muti modal function is ok\n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "\n",
    "# test_muti_modal_vectors = []\n",
    "# for idx , data in enumerate(test_features):\n",
    "#     test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "# test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "# nan_indices = np.where(np.isnan(test_features))\n",
    "# print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "# print(test_features[1])\n",
    "# print(test_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_list, test_list = dataContainer.getFilenamesList()\n",
    "\n",
    "# len(train_list), len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<contextlib._GeneratorContextManager object at 0x000001FBA8C7CE90>\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print(xgb.config_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/HAM10000/XGB/skin_vgg16_layer44\n",
      "input size is :[1, 7]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :2003\n",
      "label size is :2003\n",
      "feature size is :8012\n",
      "label size is :8012\n",
      "feature size is : (2003, 7)\n",
      "[20:01:49] INFO: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\simple_dmatrix.cc:137: Generating new Ellpack page.\n",
      "in callback class\n",
      "in callback class\n",
      "Best iteration: 1\n",
      "res's len : 8012\n",
      "feature's len : 8012\n",
      "跑訓練資料的時間:2.675888292670941e-08\n",
      "======eval finish!=========\n",
      "best acc is : -0.802796\n",
      "best f1 is : -0.794829\n",
      "XGB eval time : 2.320269645513484e-08\n",
      "model_name : layer44\n",
      "../model/HAM10000/XGB/skin_vgg16_layer43\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# training XGB !!\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     f1, acc, \u001b[38;5;28miter\u001b[39m, CNN_eval_time, XGB_eval_time \u001b[38;5;241m=\u001b[39m \u001b[43mrecordXGBoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_output_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxgb_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough memory!! escape form loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m, in \u001b[0;36mrecordXGBoutput\u001b[1;34m(model, train_dataloader, test_dataloader, enable_muti_module, isRGB, model_output_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecordXGBoutput\u001b[39m(model:nn\u001b[38;5;241m.\u001b[39mSequential, train_dataloader, test_dataloader, enable_muti_module:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, isRGB:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, model_output_path:\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcal CNN model output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m   test_features , test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdataloaderToFeatureData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     13\u001b[0m   train_features , train_labels \u001b[38;5;241m=\u001b[39m dataloaderToFeatureData(model, train_dataloader, device)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\helperFunctions.py:48\u001b[0m, in \u001b[0;36mdataloaderToFeatureData\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 48\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\CustomImageDataset.py:30\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[1;32m---> 30\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_to_label_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx]]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    865\u001b[0m ):\n\u001b[0;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[],\n",
    "       \"CNN eval time\":[],\n",
    "       \"XGB eval time\":[],\n",
    "       }\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    model = model.to(device)\n",
    "    model_name = model_name.replace(\":\",\"\")\n",
    "    xgb_model_folder = model_folder_path + \"/XGB/\"\n",
    "    xgb_output_model_name = data_select_signal + \"_\" + model_select_signal + \"_\"  +  model_name \n",
    "    xgb_model_path =  xgb_model_folder + xgb_output_model_name\n",
    "    \n",
    "    print(xgb_model_path)\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        except MemoryError:\n",
    "            print(\"Not enough memory!! escape form loop\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"error catch :{}\".format(e))\n",
    "            break\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    # training XGB !!\n",
    "    try:\n",
    "        f1, acc, iter, CNN_eval_time, XGB_eval_time = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal, model_output_path=xgb_model_path)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    # except:\n",
    "    #     print(\"error catch \")\n",
    "    #     break\n",
    "\n",
    "    print(\"best acc is : {}\".format(acc[iter]))\n",
    "    print(\"best f1 is : {}\".format(f1[iter]))\n",
    "    print(\"XGB eval time : {}\".format(XGB_eval_time) )\n",
    "\n",
    "    print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    cur_acc = acc[iter]\n",
    "    XGB_res[\"acc\"].append((-cur_acc) if cur_acc < 0 else cur_acc)\n",
    "    cur_f1 = f1[iter]\n",
    "    XGB_res[\"f1\"].append((-cur_f1) if cur_f1 < 0 else cur_f1)\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "    XGB_res[\"CNN eval time\"].append(CNN_eval_time)\n",
    "    XGB_res[\"XGB eval time\"].append(XGB_eval_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(\n",
    "    async_discord_bot_notifier(\"XGB訓練完成!\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[15:23:57] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\gbm\\gbtree.cc:130: Using tree method: 0\n",
    "[15:23:57] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:744: [GPU Hist]: Configure\n",
    "[15:23:57] INFO: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\simple_dmatrix.cc:137: Generating new Ellpack page.\n",
    "[15:23:57] ======== Monitor (0): SketchContainer ========\n",
    "[15:23:57] MakeCuts: 0.001069s, 1 calls @ 1069us\n",
    "\n",
    "[15:23:57] Prune: 0.000582s, 1 calls @ 582us\n",
    "\n",
    "[15:23:57] ScanInput: 0.000698s, 1 calls @ 698us\n",
    "\n",
    "[15:23:57] Unique: 0.000319s, 1 calls @ 319us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): Learner ========\n",
    "[15:23:59] Configure: 0s, 1 calls @ 0us\n",
    "\n",
    "[15:23:59] EvalOneIter: 0.096158s, 25 calls @ 96158us\n",
    "\n",
    "[15:23:59] GetGradient: 0.027228s, 25 calls @ 27228us\n",
    "\n",
    "[15:23:59] PredictRaw: 0.005237s, 25 calls @ 5237us\n",
    "\n",
    "[15:23:59] UpdateOneIter: 1.1023s, 25 calls @ 1102302us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GBTree ========\n",
    "[15:23:59] BoostNewTrees: 1.06878s, 25 calls @ 1068777us\n",
    "\n",
    "[15:23:59] CommitModel: 9e-05s, 25 calls @ 90us\n",
    "\n",
    "[15:23:59] ======== Device 0 Memory Allocations:  ========\n",
    "[15:23:59] Peak memory usage: 33MiB\n",
    "[15:23:59] Number of allocations: 9943\n",
    "[15:23:59] ======== Monitor (0): updater_gpu_hist ========\n",
    "[15:23:59] InitData: 0.00142s, 175 calls @ 1420us\n",
    "\n",
    "[15:23:59] InitDataOnce: 0.001368s, 1 calls @ 1368us\n",
    "\n",
    "[15:23:59] Update: 1.01252s, 175 calls @ 1012524us\n",
    "\n",
    "[15:23:59] UpdatePredictionCache: 0.040336s, 175 calls @ 40336us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): gradient_based_sampler ========\n",
    "[15:23:59] Sample: 0.075805s, 175 calls @ 75805us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GPUHistMakerDevice0 ========\n",
    "[15:23:59] AllReduce: 0.001081s, 1050 calls @ 1081us\n",
    "\n",
    "[15:23:59] BuildHist: 0.050633s, 1047 calls @ 50633us\n",
    "\n",
    "[15:23:59] EvaluateSplits: 0.280004s, 1047 calls @ 280004us\n",
    "\n",
    "[15:23:59] FinalisePosition: 0.0455s, 175 calls @ 45500us\n",
    "\n",
    "[15:23:59] InitRoot: 0.150963s, 175 calls @ 150963us\n",
    "\n",
    "[15:23:59] Reset: 0.194852s, 175 calls @ 194852us\n",
    "\n",
    "[15:23:59] UpdatePosition: 0.259407s, 1047 calls @ 259407us\n",
    "\n",
    "Best iteration: 4\n",
    "[15:23:59] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\gbm\\gbtree.cc:130: Using tree method: 0\n",
    "[15:23:59] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:744: [GPU Hist]: Configure\n",
    "[15:23:59] ======== Monitor (0): ellpack_page ========\n",
    "[15:23:59] BinningCompression: 0.01666s, 1 calls @ 16660us\n",
    "\n",
    "[15:23:59] InitCompressedData: 0.000415s, 1 calls @ 415us\n",
    "\n",
    "[15:23:59] Quantiles: 0.057138s, 1 calls @ 57138us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): Learner ========\n",
    "[15:23:59] Configure: 0.000601s, 1 calls @ 601us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GBTree ========\n",
    "[15:23:59] ======== Device 0 Memory Allocations:  ========\n",
    "[15:23:59] Peak memory usage: 33MiB\n",
    "[15:23:59] Number of allocations: 9956\n",
    "[15:23:59] ======== Monitor (0): updater_gpu_hist ========[15:23:57] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\gbm\\gbtree.cc:130: Using tree method: 0\n",
    "[15:23:57] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:744: [GPU Hist]: Configure\n",
    "[15:23:57] INFO: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\data\\simple_dmatrix.cc:137: Generating new Ellpack page.\n",
    "[15:23:57] ======== Monitor (0): SketchContainer ========\n",
    "[15:23:57] MakeCuts: 0.001069s, 1 calls @ 1069us\n",
    "\n",
    "[15:23:57] Prune: 0.000582s, 1 calls @ 582us\n",
    "\n",
    "[15:23:57] ScanInput: 0.000698s, 1 calls @ 698us\n",
    "\n",
    "[15:23:57] Unique: 0.000319s, 1 calls @ 319us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): Learner ========\n",
    "[15:23:59] Configure: 0s, 1 calls @ 0us\n",
    "\n",
    "[15:23:59] EvalOneIter: 0.096158s, 25 calls @ 96158us\n",
    "\n",
    "[15:23:59] GetGradient: 0.027228s, 25 calls @ 27228us\n",
    "\n",
    "[15:23:59] PredictRaw: 0.005237s, 25 calls @ 5237us\n",
    "\n",
    "[15:23:59] UpdateOneIter: 1.1023s, 25 calls @ 1102302us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GBTree ========\n",
    "[15:23:59] BoostNewTrees: 1.06878s, 25 calls @ 1068777us\n",
    "\n",
    "[15:23:59] CommitModel: 9e-05s, 25 calls @ 90us\n",
    "\n",
    "[15:23:59] ======== Device 0 Memory Allocations:  ========\n",
    "[15:23:59] Peak memory usage: 33MiB\n",
    "[15:23:59] Number of allocations: 9943\n",
    "[15:23:59] ======== Monitor (0): updater_gpu_hist ========\n",
    "[15:23:59] InitData: 0.00142s, 175 calls @ 1420us\n",
    "\n",
    "[15:23:59] InitDataOnce: 0.001368s, 1 calls @ 1368us\n",
    "\n",
    "[15:23:59] Update: 1.01252s, 175 calls @ 1012524us\n",
    "\n",
    "[15:23:59] UpdatePredictionCache: 0.040336s, 175 calls @ 40336us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): gradient_based_sampler ========\n",
    "[15:23:59] Sample: 0.075805s, 175 calls @ 75805us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GPUHistMakerDevice0 ========\n",
    "[15:23:59] AllReduce: 0.001081s, 1050 calls @ 1081us\n",
    "\n",
    "[15:23:59] BuildHist: 0.050633s, 1047 calls @ 50633us\n",
    "\n",
    "[15:23:59] EvaluateSplits: 0.280004s, 1047 calls @ 280004us\n",
    "\n",
    "[15:23:59] FinalisePosition: 0.0455s, 175 calls @ 45500us\n",
    "\n",
    "[15:23:59] InitRoot: 0.150963s, 175 calls @ 150963us\n",
    "\n",
    "[15:23:59] Reset: 0.194852s, 175 calls @ 194852us\n",
    "\n",
    "[15:23:59] UpdatePosition: 0.259407s, 1047 calls @ 259407us\n",
    "\n",
    "Best iteration: 4\n",
    "[15:23:59] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\gbm\\gbtree.cc:130: Using tree method: 0\n",
    "[15:23:59] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\tree\\updater_gpu_hist.cu:744: [GPU Hist]: Configure\n",
    "[15:23:59] ======== Monitor (0): ellpack_page ========\n",
    "[15:23:59] BinningCompression: 0.01666s, 1 calls @ 16660us\n",
    "\n",
    "[15:23:59] InitCompressedData: 0.000415s, 1 calls @ 415us\n",
    "\n",
    "[15:23:59] Quantiles: 0.057138s, 1 calls @ 57138us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): Learner ========\n",
    "[15:23:59] Configure: 0.000601s, 1 calls @ 601us\n",
    "\n",
    "[15:23:59] ======== Monitor (0): GBTree ========\n",
    "[15:23:59] ======== Device 0 Memory Allocations:  ========\n",
    "[15:23:59] Peak memory usage: 33MiB\n",
    "[15:23:59] Number of allocations: 9956\n",
    "[15:23:59] ======== Monitor (0): updater_gpu_hist ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[16:09:44] ======== Monitor (0): HostSketchContainer ========\n",
    "[16:09:44] AllReduce: 0.001803s, 1 calls @ 1803us\n",
    "\n",
    "[16:09:44] MakeCuts: 0.002202s, 1 calls @ 2202us\n",
    "\n",
    "[16:09:44] PushRowPage: 0.000832s, 1 calls @ 832us\n",
    "\n",
    "[16:09:45] ======== Monitor (0): Learner ========\n",
    "[16:09:45] Configure: 0s, 1 calls @ 0us\n",
    "\n",
    "[16:09:45] EvalOneIter: 0.030946s, 24 calls @ 30946us\n",
    "\n",
    "[16:09:45] GetGradient: 0.025382s, 24 calls @ 25382us\n",
    "\n",
    "[16:09:45] PredictRaw: 0.000549s, 24 calls @ 549us\n",
    "\n",
    "[16:09:45] UpdateOneIter: 0.326182s, 24 calls @ 326182us\n",
    "\n",
    "[16:09:45] ======== Monitor (0): GBTree ========\n",
    "[16:09:45] BoostNewTrees: 0.299856s, 24 calls @ 299856us\n",
    "\n",
    "[16:09:45] CommitModel: 3.7e-05s, 24 calls @ 37us\n",
    "\n",
    "[16:09:45] ======== Monitor (0): HistUpdater ========\n",
    "[16:09:45] BuildHistogram: 0.074116s, 840 calls @ 74116us\n",
    "\n",
    "[16:09:45] EvaluateSplits: 0.100393s, 1008 calls @ 100393us\n",
    "\n",
    "[16:09:45] InitData: 0.01418s, 168 calls @ 14180us\n",
    "\n",
    "[16:09:45] InitRoot: 0.032161s, 168 calls @ 32161us\n",
    "\n",
    "[16:09:45] LeafPartition: 4.1e-05s, 168 calls @ 41us\n",
    "\n",
    "[16:09:45] UpdatePosition: 0.047378s, 1008 calls @ 47378us\n",
    "\n",
    "[16:09:45] UpdatePredictionCache: 0.007718s, 168 calls @ 7718us\n",
    "\n",
    "[16:09:45] UpdateTree: 0.276459s, 168 calls @ 276459us\n",
    "\n",
    "Best iteration: 3\n",
    "[16:09:45] DEBUG: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\gbm\\gbtree.cc:130: Using tree method: 0\n",
    "[16:09:45] ======== Monitor (0): Learner ========\n",
    "[16:09:45] Configure: 0.000339s, 1 calls @ 339us\n",
    "\n",
    "[16:09:45] ======== Monitor (0): GBTree ========\n",
    "[16:09:45] ======== Monitor (0):  ========"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 0]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "label size is :2003\n",
      "feature size is :8012\n",
      "label size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 12.5224 秒\n",
      "預測時間 in 0.1399 秒\n",
      "預測時間 in 0.0430 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9999 , 0.9999。\n",
      "測試集的 F1 score和acc分別為: 0.5422 , 0.6735。\n",
      "======eval finish!=========\n",
      "layer:2\n",
      "input size is :[1, 3, 244, 244]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "label size is :2003\n",
      "feature size is :8012\n",
      "label size is :8012\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput size is :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_size))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mrecordMLoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#     print(\"ERROR!!!\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# cal output size\u001b[39;00m\n\u001b[0;32m     49\u001b[0m linear_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "Cell \u001b[1;32mIn[10], line 24\u001b[0m, in \u001b[0;36mrecordMLoutput\u001b[1;34m(ML_model, model, train_dataloader, test_dataloader, enable_muti_module)\u001b[0m\n\u001b[0;32m     20\u001b[0m nan_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(np\u001b[38;5;241m.\u001b[39misnan(test_features))\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(len(test_features[0]))\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======eval finish!=========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f1, acc\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\XgbHelperFunction.py:18\u001b[0m, in \u001b[0;36mtrain_predict\u001b[1;34m(clf, X_train, y_train, X_test, y_test, evalByMMSE)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m 模型，樣本數: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m。\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(clf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train)))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 在訓練集上評估模型\u001b[39;00m\n\u001b[0;32m     20\u001b[0m res1 , res2 \u001b[38;5;241m=\u001b[39m predict_labels(clf, X_train, y_train, evalByMMSE)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\XgbHelperFunction.py:65\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(clf, X_train, y_train)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 紀錄訓練時間\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(\"訓練資料 : \".format(X_train[0:4]))\u001b[39;00m\n\u001b[0;32m     64\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練時間 \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB105\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"ERROR!!!\")\n",
    "            break\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # try:\n",
    "        f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "        # except:\n",
    "        #     print(\"ERROR!!!\")\n",
    "        #     break\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    \n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" +  model_select_signal + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(async_discord_bot_notifier(\"ML model訓練完成!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
