{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import asyncio ,nest_asyncio\n",
    "from python_tools.discord_bot import async_discord_bot_notifier ,discord_bot_notifier\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_select_signal = 'resnet18'\n",
    "# model_select_signal = 'vgg16'\n",
    "\n",
    "data_select_signal = 'skin'\n",
    "# data_select_signal = 'chest CT'\n",
    "\n",
    "# select image type is RGB or not\n",
    "isRGB = True\n",
    "# isRGB = False\n",
    "\n",
    "# enable img feature vector as mutimodal\n",
    "enable_muti_modal_signal = True\n",
    "# enable_muti_modal_signal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'chest CT':\n",
    "    dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "else:\n",
    "    raise ValueError('需要指定dataset類別')\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.returnDataloaders()\n",
    "train_files , test_files = dataContainer.returnDatasetFilenames()\n",
    "feature_vector_file_path = dataContainer.returnFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cal img feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\skimage\\feature\\texture.py:353: UserWarning: Applying `local_binary_pattern` to floating-point images may give unexpected results when small numerical differences between adjacent pixels are present. It is recommended to use this function with images of integer dtype.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "train_img_feature_vector = []\n",
    "test_img_feature_vector = []\n",
    "\n",
    "def print_list_dimensions(lst):\n",
    "    dimensions = []\n",
    "    while isinstance(lst, list):\n",
    "        dimensions.append(len(lst))\n",
    "        lst = lst[0] if len(lst) > 0 else []\n",
    "    print(\"Dimensions:\", \" x \".join(map(str, dimensions)))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if enable_muti_modal_signal:\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(train_dataloader):\n",
    "        for img in batch_data:\n",
    "            train_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(train_img_feature_vector)\n",
    "\n",
    "    for idx , (batch_data, label) in enumerate(test_dataloader):\n",
    "        for img in batch_data:\n",
    "            test_img_feature_vector.append(calImgFeatureVector(img, isRGB=isRGB))\n",
    "    \n",
    "    # print_list_dimensions(test_feature_vector)\n",
    "        \n",
    "    # for idx , (data, label) in enumerate(test_dataloader):\n",
    "    #     train_img_feature_vector.append( calImgFeatureVector(data, isRGB=isRGB) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01753672 0.12859136 0.14988535 0.12289615 0.27123658 0.19446339\n",
      " 0.26082893 0.05883291 0.06812022 0.08902663 0.14245855 0.17151626\n",
      " 0.14090402 0.09044165 0.06666534 0.0598294  0.11220504 0.30736113\n",
      " 0.21707693 0.88701373 0.26760662 0.         0.         0.\n",
      " 0.         0.77097738 0.62835687 0.1037376  0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(test_img_feature_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module:bool=False, isRGB:bool=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  \n",
    "  start_time = time.time()\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "  end_time = time.time()\n",
    "  CNN_eval_time = (end_time - start_time)/len(train_features)\n",
    "\n",
    "  # muti modal selection\n",
    "  if enable_muti_module:\n",
    "\n",
    "    train_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(train_features):\n",
    "      train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "\n",
    "\n",
    "  print(\"feature size is : {}\".format(test_features.shape))\n",
    "  # print_list_dimensions(test_features)\n",
    "  \n",
    "  print(feature for feature in test_features if len(feature) == 0)\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "  print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "\n",
    "  \n",
    "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  iter ,f1 ,acc, XGB_eval_time = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, enable_f1_metric=True)\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc, iter, CNN_eval_time, XGB_eval_time\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  str_time = time.time()\n",
    "  # train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "\n",
    "  if enable_muti_module:\n",
    "    # train_muti_modal_vectors = []\n",
    "    # for idx , data in enumerate(train_features):\n",
    "    #   train_muti_modal_vectors.append(np.concatenate((train_features[idx] , train_img_feature_vector[idx])))\n",
    "    # train_features = np.array(train_muti_modal_vectors)\n",
    "\n",
    "    test_muti_modal_vectors = []\n",
    "    for idx , data in enumerate(test_features):\n",
    "      test_muti_modal_vectors.append(np.concatenate((test_features[idx] , test_img_feature_vector[idx])))\n",
    "    test_features = np.array(test_muti_modal_vectors)\n",
    "\n",
    "  # print(\"????\")\n",
    "  # print(test_img_feature_vector[0])\n",
    "  print(feature for feature in test_features if len(feature) == 0)\n",
    "  nan_indices = np.where(np.isnan(test_features))\n",
    "  print(f'Indices of NaN values: {list(zip(nan_indices[0], nan_indices[1]))}')\n",
    "  \n",
    "  # print(len(test_features[0]))\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 7]                    --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 7]                    --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 7]                    --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-7                  [1, 7]                    28,679\n",
       "==========================================================================================\n",
       "Total params: 134,289,223\n",
       "Trainable params: 134,289,223\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.16\n",
       "Estimated Total Size (MB): 646.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    model_folder_path = \"../model/HAM10000\"\n",
    "elif data_select_signal == 'chest CT':\n",
    "    model_folder_path = \"../model/CT chest\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "summary(vgg16, [1,3,224,224])\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "model_0 = None\n",
    "\n",
    "if model_select_signal == 'resnet18':\n",
    "    model_0 = resnet18\n",
    "elif model_select_signal == 'vgg16':\n",
    "    model_0 = vgg16\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :2003\n",
      "feature size is :8012\n"
     ]
    }
   ],
   "source": [
    "test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "\n",
    "# for idx , data in enumerate(train_features):\n",
    "#     train_features[idx] = np.concatenate(train_features[idx] , train_img_feature_vector[idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "# print(len(train_features))\n",
    "# print(type(train_features))\n",
    "# print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 初始化层计数器\n",
    "# total_layers = 0\n",
    "\n",
    "# # 遍历模型的所有子模块和层\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # 打印每一层的名称和它的具体类型，这一步是可选的，但对理解模型结构很有帮助\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "#     # 对所有层进行计数（包括卷积层、全连接层等）\n",
    "#     # 如果只想计算特定类型的层（如卷积层Conv2d），则需要添加判断条件\n",
    "#     total_layers += 1\n",
    "\n",
    "# # 打印总层数\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# # 示例：仅计算Conv2d层的数量\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 68層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5025377235715351, 0.8467000835421888, 0.8393717787047136)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.helperFunctions import calDetailModelLayersNum\n",
    "\n",
    "\n",
    "# summary(list_of_models[23][0], input_size=[1,3,224,224],depth=4)\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0)):\n",
    "#     try : \n",
    "#         summary(list_of_models[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 512, 7, 7]            --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "==========================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 85.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[2][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "feature size is : (2003, 48)\n",
      "<generator object recordXGBoutput.<locals>.<genexpr> at 0x000002D023828AC0>\n",
      "Indices of NaN values: [(1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (34, 9), (34, 10), (34, 11), (34, 12), (34, 13), (39, 9), (39, 10), (39, 11), (39, 12), (39, 13), (46, 9), (46, 10), (46, 11), (46, 12), (46, 13), (77, 9), (77, 10), (77, 11), (77, 12), (77, 13), (84, 9), (84, 10), (84, 11), (84, 12), (84, 13), (136, 9), (136, 10), (136, 11), (136, 12), (136, 13), (138, 9), (138, 10), (138, 11), (138, 12), (138, 13), (157, 9), (157, 10), (157, 11), (157, 12), (157, 13), (164, 9), (164, 10), (164, 11), (164, 12), (164, 13), (166, 9), (166, 10), (166, 11), (166, 12), (166, 13), (177, 9), (177, 10), (177, 11), (177, 12), (177, 13), (179, 9), (179, 10), (179, 11), (179, 12), (179, 13), (230, 9), (230, 10), (230, 11), (230, 12), (230, 13), (232, 9), (232, 10), (232, 11), (232, 12), (232, 13), (236, 9), (236, 10), (236, 11), (236, 12), (236, 13), (243, 9), (243, 10), (243, 11), (243, 12), (243, 13), (251, 9), (251, 10), (251, 11), (251, 12), (251, 13), (274, 9), (274, 10), (274, 11), (274, 12), (274, 13), (276, 9), (276, 10), (276, 11), (276, 12), (276, 13), (283, 9), (283, 10), (283, 11), (283, 12), (283, 13), (297, 9), (297, 10), (297, 11), (297, 12), (297, 13), (299, 9), (299, 10), (299, 11), (299, 12), (299, 13), (318, 9), (318, 10), (318, 11), (318, 12), (318, 13), (321, 9), (321, 10), (321, 11), (321, 12), (321, 13), (357, 9), (357, 10), (357, 11), (357, 12), (357, 13), (373, 9), (373, 10), (373, 11), (373, 12), (373, 13), (380, 9), (380, 10), (380, 11), (380, 12), (380, 13), (387, 9), (387, 10), (387, 11), (387, 12), (387, 13), (397, 9), (397, 10), (397, 11), (397, 12), (397, 13), (422, 9), (422, 10), (422, 11), (422, 12), (422, 13), (424, 9), (424, 10), (424, 11), (424, 12), (424, 13), (427, 9), (427, 10), (427, 11), (427, 12), (427, 13), (431, 9), (431, 10), (431, 11), (431, 12), (431, 13), (434, 9), (434, 10), (434, 11), (434, 12), (434, 13), (441, 9), (441, 10), (441, 11), (441, 12), (441, 13), (451, 9), (451, 10), (451, 11), (451, 12), (451, 13), (458, 9), (458, 10), (458, 11), (458, 12), (458, 13), (489, 9), (489, 10), (489, 11), (489, 12), (489, 13), (494, 9), (494, 10), (494, 11), (494, 12), (494, 13), (495, 9), (495, 10), (495, 11), (495, 12), (495, 13), (496, 9), (496, 10), (496, 11), (496, 12), (496, 13), (510, 9), (510, 10), (510, 11), (510, 12), (510, 13), (512, 9), (512, 10), (512, 11), (512, 12), (512, 13), (517, 9), (517, 10), (517, 11), (517, 12), (517, 13), (519, 9), (519, 10), (519, 11), (519, 12), (519, 13), (532, 9), (532, 10), (532, 11), (532, 12), (532, 13), (540, 9), (540, 10), (540, 11), (540, 12), (540, 13), (548, 9), (548, 10), (548, 11), (548, 12), (548, 13), (555, 9), (555, 10), (555, 11), (555, 12), (555, 13), (560, 9), (560, 10), (560, 11), (560, 12), (560, 13), (565, 9), (565, 10), (565, 11), (565, 12), (565, 13), (577, 9), (577, 10), (577, 11), (577, 12), (577, 13), (595, 9), (595, 10), (595, 11), (595, 12), (595, 13), (604, 9), (604, 10), (604, 11), (604, 12), (604, 13), (605, 9), (605, 10), (605, 11), (605, 12), (605, 13), (636, 9), (636, 10), (636, 11), (636, 12), (636, 13), (641, 9), (641, 10), (641, 11), (641, 12), (641, 13), (647, 9), (647, 10), (647, 11), (647, 12), (647, 13), (656, 9), (656, 10), (656, 11), (656, 12), (656, 13), (660, 9), (660, 10), (660, 11), (660, 12), (660, 13), (661, 9), (661, 10), (661, 11), (661, 12), (661, 13), (666, 9), (666, 10), (666, 11), (666, 12), (666, 13), (681, 9), (681, 10), (681, 11), (681, 12), (681, 13), (689, 9), (689, 10), (689, 11), (689, 12), (689, 13), (691, 9), (691, 10), (691, 11), (691, 12), (691, 13), (694, 9), (694, 10), (694, 11), (694, 12), (694, 13), (700, 9), (700, 10), (700, 11), (700, 12), (700, 13), (705, 9), (705, 10), (705, 11), (705, 12), (705, 13), (707, 9), (707, 10), (707, 11), (707, 12), (707, 13), (719, 9), (719, 10), (719, 11), (719, 12), (719, 13), (721, 9), (721, 10), (721, 11), (721, 12), (721, 13), (732, 9), (732, 10), (732, 11), (732, 12), (732, 13), (738, 9), (738, 10), (738, 11), (738, 12), (738, 13), (741, 9), (741, 10), (741, 11), (741, 12), (741, 13), (760, 9), (760, 10), (760, 11), (760, 12), (760, 13), (766, 9), (766, 10), (766, 11), (766, 12), (766, 13), (772, 9), (772, 10), (772, 11), (772, 12), (772, 13), (775, 9), (775, 10), (775, 11), (775, 12), (775, 13), (776, 9), (776, 10), (776, 11), (776, 12), (776, 13), (800, 9), (800, 10), (800, 11), (800, 12), (800, 13), (802, 9), (802, 10), (802, 11), (802, 12), (802, 13), (815, 9), (815, 10), (815, 11), (815, 12), (815, 13), (825, 9), (825, 10), (825, 11), (825, 12), (825, 13), (830, 9), (830, 10), (830, 11), (830, 12), (830, 13), (831, 9), (831, 10), (831, 11), (831, 12), (831, 13), (842, 9), (842, 10), (842, 11), (842, 12), (842, 13), (844, 9), (844, 10), (844, 11), (844, 12), (844, 13), (845, 9), (845, 10), (845, 11), (845, 12), (845, 13), (865, 9), (865, 10), (865, 11), (865, 12), (865, 13), (868, 9), (868, 10), (868, 11), (868, 12), (868, 13), (869, 9), (869, 10), (869, 11), (869, 12), (869, 13), (872, 9), (872, 10), (872, 11), (872, 12), (872, 13), (877, 9), (877, 10), (877, 11), (877, 12), (877, 13), (890, 9), (890, 10), (890, 11), (890, 12), (890, 13), (899, 9), (899, 10), (899, 11), (899, 12), (899, 13), (900, 9), (900, 10), (900, 11), (900, 12), (900, 13), (915, 9), (915, 10), (915, 11), (915, 12), (915, 13), (930, 9), (930, 10), (930, 11), (930, 12), (930, 13), (933, 9), (933, 10), (933, 11), (933, 12), (933, 13), (938, 9), (938, 10), (938, 11), (938, 12), (938, 13), (944, 9), (944, 10), (944, 11), (944, 12), (944, 13), (949, 9), (949, 10), (949, 11), (949, 12), (949, 13), (957, 9), (957, 10), (957, 11), (957, 12), (957, 13), (969, 9), (969, 10), (969, 11), (969, 12), (969, 13), (970, 9), (970, 10), (970, 11), (970, 12), (970, 13), (975, 9), (975, 10), (975, 11), (975, 12), (975, 13), (995, 9), (995, 10), (995, 11), (995, 12), (995, 13), (998, 9), (998, 10), (998, 11), (998, 12), (998, 13), (1000, 9), (1000, 10), (1000, 11), (1000, 12), (1000, 13), (1010, 9), (1010, 10), (1010, 11), (1010, 12), (1010, 13), (1012, 9), (1012, 10), (1012, 11), (1012, 12), (1012, 13), (1015, 9), (1015, 10), (1015, 11), (1015, 12), (1015, 13), (1018, 9), (1018, 10), (1018, 11), (1018, 12), (1018, 13), (1071, 9), (1071, 10), (1071, 11), (1071, 12), (1071, 13), (1073, 9), (1073, 10), (1073, 11), (1073, 12), (1073, 13), (1080, 9), (1080, 10), (1080, 11), (1080, 12), (1080, 13), (1102, 9), (1102, 10), (1102, 11), (1102, 12), (1102, 13), (1108, 9), (1108, 10), (1108, 11), (1108, 12), (1108, 13), (1114, 9), (1114, 10), (1114, 11), (1114, 12), (1114, 13), (1121, 9), (1121, 10), (1121, 11), (1121, 12), (1121, 13), (1129, 9), (1129, 10), (1129, 11), (1129, 12), (1129, 13), (1136, 9), (1136, 10), (1136, 11), (1136, 12), (1136, 13), (1145, 9), (1145, 10), (1145, 11), (1145, 12), (1145, 13), (1146, 9), (1146, 10), (1146, 11), (1146, 12), (1146, 13), (1172, 9), (1172, 10), (1172, 11), (1172, 12), (1172, 13), (1186, 9), (1186, 10), (1186, 11), (1186, 12), (1186, 13), (1193, 9), (1193, 10), (1193, 11), (1193, 12), (1193, 13), (1196, 9), (1196, 10), (1196, 11), (1196, 12), (1196, 13), (1203, 9), (1203, 10), (1203, 11), (1203, 12), (1203, 13), (1213, 9), (1213, 10), (1213, 11), (1213, 12), (1213, 13), (1216, 9), (1216, 10), (1216, 11), (1216, 12), (1216, 13), (1222, 9), (1222, 10), (1222, 11), (1222, 12), (1222, 13), (1245, 9), (1245, 10), (1245, 11), (1245, 12), (1245, 13), (1272, 9), (1272, 10), (1272, 11), (1272, 12), (1272, 13), (1273, 9), (1273, 10), (1273, 11), (1273, 12), (1273, 13), (1281, 9), (1281, 10), (1281, 11), (1281, 12), (1281, 13), (1287, 9), (1287, 10), (1287, 11), (1287, 12), (1287, 13), (1288, 9), (1288, 10), (1288, 11), (1288, 12), (1288, 13), (1292, 9), (1292, 10), (1292, 11), (1292, 12), (1292, 13), (1295, 9), (1295, 10), (1295, 11), (1295, 12), (1295, 13), (1300, 9), (1300, 10), (1300, 11), (1300, 12), (1300, 13), (1301, 9), (1301, 10), (1301, 11), (1301, 12), (1301, 13), (1306, 9), (1306, 10), (1306, 11), (1306, 12), (1306, 13), (1309, 9), (1309, 10), (1309, 11), (1309, 12), (1309, 13), (1322, 9), (1322, 10), (1322, 11), (1322, 12), (1322, 13), (1340, 9), (1340, 10), (1340, 11), (1340, 12), (1340, 13), (1358, 9), (1358, 10), (1358, 11), (1358, 12), (1358, 13), (1364, 9), (1364, 10), (1364, 11), (1364, 12), (1364, 13), (1370, 9), (1370, 10), (1370, 11), (1370, 12), (1370, 13), (1374, 9), (1374, 10), (1374, 11), (1374, 12), (1374, 13), (1382, 9), (1382, 10), (1382, 11), (1382, 12), (1382, 13), (1388, 9), (1388, 10), (1388, 11), (1388, 12), (1388, 13), (1394, 9), (1394, 10), (1394, 11), (1394, 12), (1394, 13), (1396, 9), (1396, 10), (1396, 11), (1396, 12), (1396, 13), (1397, 9), (1397, 10), (1397, 11), (1397, 12), (1397, 13), (1407, 9), (1407, 10), (1407, 11), (1407, 12), (1407, 13), (1420, 9), (1420, 10), (1420, 11), (1420, 12), (1420, 13), (1461, 9), (1461, 10), (1461, 11), (1461, 12), (1461, 13), (1483, 9), (1483, 10), (1483, 11), (1483, 12), (1483, 13), (1488, 9), (1488, 10), (1488, 11), (1488, 12), (1488, 13), (1490, 9), (1490, 10), (1490, 11), (1490, 12), (1490, 13), (1492, 9), (1492, 10), (1492, 11), (1492, 12), (1492, 13), (1494, 9), (1494, 10), (1494, 11), (1494, 12), (1494, 13), (1514, 9), (1514, 10), (1514, 11), (1514, 12), (1514, 13), (1530, 9), (1530, 10), (1530, 11), (1530, 12), (1530, 13), (1537, 9), (1537, 10), (1537, 11), (1537, 12), (1537, 13), (1539, 9), (1539, 10), (1539, 11), (1539, 12), (1539, 13), (1545, 9), (1545, 10), (1545, 11), (1545, 12), (1545, 13), (1548, 9), (1548, 10), (1548, 11), (1548, 12), (1548, 13), (1555, 9), (1555, 10), (1555, 11), (1555, 12), (1555, 13), (1564, 9), (1564, 10), (1564, 11), (1564, 12), (1564, 13), (1568, 9), (1568, 10), (1568, 11), (1568, 12), (1568, 13), (1577, 9), (1577, 10), (1577, 11), (1577, 12), (1577, 13), (1580, 9), (1580, 10), (1580, 11), (1580, 12), (1580, 13), (1592, 9), (1592, 10), (1592, 11), (1592, 12), (1592, 13), (1612, 9), (1612, 10), (1612, 11), (1612, 12), (1612, 13), (1626, 9), (1626, 10), (1626, 11), (1626, 12), (1626, 13), (1634, 9), (1634, 10), (1634, 11), (1634, 12), (1634, 13), (1646, 9), (1646, 10), (1646, 11), (1646, 12), (1646, 13), (1656, 9), (1656, 10), (1656, 11), (1656, 12), (1656, 13), (1658, 9), (1658, 10), (1658, 11), (1658, 12), (1658, 13), (1662, 9), (1662, 10), (1662, 11), (1662, 12), (1662, 13), (1667, 9), (1667, 10), (1667, 11), (1667, 12), (1667, 13), (1668, 9), (1668, 10), (1668, 11), (1668, 12), (1668, 13), (1670, 9), (1670, 10), (1670, 11), (1670, 12), (1670, 13), (1679, 9), (1679, 10), (1679, 11), (1679, 12), (1679, 13), (1689, 9), (1689, 10), (1689, 11), (1689, 12), (1689, 13), (1692, 9), (1692, 10), (1692, 11), (1692, 12), (1692, 13), (1694, 9), (1694, 10), (1694, 11), (1694, 12), (1694, 13), (1705, 9), (1705, 10), (1705, 11), (1705, 12), (1705, 13), (1706, 9), (1706, 10), (1706, 11), (1706, 12), (1706, 13), (1708, 9), (1708, 10), (1708, 11), (1708, 12), (1708, 13), (1712, 9), (1712, 10), (1712, 11), (1712, 12), (1712, 13), (1722, 9), (1722, 10), (1722, 11), (1722, 12), (1722, 13), (1739, 9), (1739, 10), (1739, 11), (1739, 12), (1739, 13), (1740, 9), (1740, 10), (1740, 11), (1740, 12), (1740, 13), (1745, 9), (1745, 10), (1745, 11), (1745, 12), (1745, 13), (1748, 9), (1748, 10), (1748, 11), (1748, 12), (1748, 13), (1750, 9), (1750, 10), (1750, 11), (1750, 12), (1750, 13), (1756, 9), (1756, 10), (1756, 11), (1756, 12), (1756, 13), (1767, 9), (1767, 10), (1767, 11), (1767, 12), (1767, 13), (1771, 9), (1771, 10), (1771, 11), (1771, 12), (1771, 13), (1806, 9), (1806, 10), (1806, 11), (1806, 12), (1806, 13), (1815, 9), (1815, 10), (1815, 11), (1815, 12), (1815, 13), (1824, 9), (1824, 10), (1824, 11), (1824, 12), (1824, 13), (1831, 9), (1831, 10), (1831, 11), (1831, 12), (1831, 13), (1835, 9), (1835, 10), (1835, 11), (1835, 12), (1835, 13), (1841, 9), (1841, 10), (1841, 11), (1841, 12), (1841, 13), (1842, 9), (1842, 10), (1842, 11), (1842, 12), (1842, 13), (1860, 9), (1860, 10), (1860, 11), (1860, 12), (1860, 13), (1864, 9), (1864, 10), (1864, 11), (1864, 12), (1864, 13), (1866, 9), (1866, 10), (1866, 11), (1866, 12), (1866, 13), (1878, 9), (1878, 10), (1878, 11), (1878, 12), (1878, 13), (1883, 9), (1883, 10), (1883, 11), (1883, 12), (1883, 13), (1890, 9), (1890, 10), (1890, 11), (1890, 12), (1890, 13), (1892, 9), (1892, 10), (1892, 11), (1892, 12), (1892, 13), (1895, 9), (1895, 10), (1895, 11), (1895, 12), (1895, 13), (1896, 9), (1896, 10), (1896, 11), (1896, 12), (1896, 13), (1907, 9), (1907, 10), (1907, 11), (1907, 12), (1907, 13), (1909, 9), (1909, 10), (1909, 11), (1909, 12), (1909, 13), (1915, 9), (1915, 10), (1915, 11), (1915, 12), (1915, 13), (1917, 9), (1917, 10), (1917, 11), (1917, 12), (1917, 13), (1922, 9), (1922, 10), (1922, 11), (1922, 12), (1922, 13), (1927, 9), (1927, 10), (1927, 11), (1927, 12), (1927, 13), (1940, 9), (1940, 10), (1940, 11), (1940, 12), (1940, 13), (1944, 9), (1944, 10), (1944, 11), (1944, 12), (1944, 13), (1947, 9), (1947, 10), (1947, 11), (1947, 12), (1947, 13), (1949, 9), (1949, 10), (1949, 11), (1949, 12), (1949, 13), (1960, 9), (1960, 10), (1960, 11), (1960, 12), (1960, 13), (1962, 9), (1962, 10), (1962, 11), (1962, 12), (1962, 13), (1963, 9), (1963, 10), (1963, 11), (1963, 12), (1963, 13), (1973, 9), (1973, 10), (1973, 11), (1973, 12), (1973, 13), (1989, 9), (1989, 10), (1989, 11), (1989, 12), (1989, 13), (1990, 9), (1990, 10), (1990, 11), (1990, 12), (1990, 13), (1995, 9), (1995, 10), (1995, 11), (1995, 12), (1995, 13)]\n",
      "Best iteration: 50\n",
      "======eval finish!=========\n",
      "best acc is : -0.83974\n",
      "best f1 is : -0.836494\n",
      "XGB eval time : 1.6225675791427604e-06\n",
      "model_name : layer:68\n",
      "input size is :[1, 512, 1, 1]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# training XGB !!\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     f1, acc, \u001b[38;5;28miter\u001b[39m, CNN_eval_time, XGB_eval_time \u001b[38;5;241m=\u001b[39m \u001b[43mrecordXGBoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough memory!! escape form loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 9\u001b[0m, in \u001b[0;36mrecordXGBoutput\u001b[1;34m(model, train_dataloader, test_dataloader, enable_muti_module, isRGB)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecordXGBoutput\u001b[39m(model:nn\u001b[38;5;241m.\u001b[39mSequential, train_dataloader, test_dataloader, enable_muti_module:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, isRGB:\u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcal CNN model output...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m   test_features , test_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdataloaderToFeatureData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m   train_features , train_labels \u001b[38;5;241m=\u001b[39m dataloaderToFeatureData(model, train_dataloader, device)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\helperFunctions.py:46\u001b[0m, in \u001b[0;36mdataloaderToFeatureData\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     45\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 46\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\CustomImageDataset.py:34\u001b[0m, in \u001b[0;36mCustomImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     32\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_to_label_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_files[idx]]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 34\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, label\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    488\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:2174\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2166\u001b[0m             \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2167\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2168\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2169\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2170\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2171\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2172\u001b[0m         )\n\u001b[1;32m-> 2174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[],\n",
    "       \"CNN eval time\":[],\n",
    "       \"XGB eval time\":[],\n",
    "       }\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        except MemoryError:\n",
    "            print(\"Not enough memory!! escape form loop\")\n",
    "            break\n",
    "        except:\n",
    "            print(\"error catch \")\n",
    "            break\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    # training XGB !!\n",
    "    try:\n",
    "        f1, acc, iter, CNN_eval_time, XGB_eval_time = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    # except:\n",
    "    #     print(\"error catch \")\n",
    "    #     break\n",
    "\n",
    "    print(\"best acc is : {}\".format(acc[iter]))\n",
    "    print(\"best f1 is : {}\".format(f1[iter]))\n",
    "    print(\"XGB eval time : {}\".format(XGB_eval_time) )\n",
    "\n",
    "    print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    cur_acc = acc[iter]\n",
    "    XGB_res[\"acc\"].append((-cur_acc) if cur_acc < 0 else cur_acc)\n",
    "    cur_f1 = f1[iter]\n",
    "    XGB_res[\"f1\"].append((-cur_f1) if cur_f1 < 0 else cur_f1)\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "    XGB_res[\"CNN eval time\"].append(CNN_eval_time)\n",
    "    XGB_res[\"XGB eval time\"].append(XGB_eval_time)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 12:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "asyncio.run(\n",
    "    async_discord_bot_notifier(\"XGB訓練完成!\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "<generator object recordMLoutput.<locals>.<genexpr> at 0x000002D02382B3E0>\n",
      "Indices of NaN values: [(1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (8, 9), (8, 10), (8, 11), (8, 12), (8, 13), (16, 9), (16, 10), (16, 11), (16, 12), (16, 13), (24, 9), (24, 10), (24, 11), (24, 12), (24, 13), (34, 9), (34, 10), (34, 11), (34, 12), (34, 13), (39, 9), (39, 10), (39, 11), (39, 12), (39, 13), (46, 9), (46, 10), (46, 11), (46, 12), (46, 13), (77, 9), (77, 10), (77, 11), (77, 12), (77, 13), (84, 9), (84, 10), (84, 11), (84, 12), (84, 13), (136, 9), (136, 10), (136, 11), (136, 12), (136, 13), (138, 9), (138, 10), (138, 11), (138, 12), (138, 13), (157, 9), (157, 10), (157, 11), (157, 12), (157, 13), (164, 9), (164, 10), (164, 11), (164, 12), (164, 13), (166, 9), (166, 10), (166, 11), (166, 12), (166, 13), (177, 9), (177, 10), (177, 11), (177, 12), (177, 13), (179, 9), (179, 10), (179, 11), (179, 12), (179, 13), (230, 9), (230, 10), (230, 11), (230, 12), (230, 13), (232, 9), (232, 10), (232, 11), (232, 12), (232, 13), (236, 9), (236, 10), (236, 11), (236, 12), (236, 13), (243, 9), (243, 10), (243, 11), (243, 12), (243, 13), (251, 9), (251, 10), (251, 11), (251, 12), (251, 13), (274, 9), (274, 10), (274, 11), (274, 12), (274, 13), (276, 9), (276, 10), (276, 11), (276, 12), (276, 13), (283, 9), (283, 10), (283, 11), (283, 12), (283, 13), (297, 9), (297, 10), (297, 11), (297, 12), (297, 13), (299, 9), (299, 10), (299, 11), (299, 12), (299, 13), (318, 9), (318, 10), (318, 11), (318, 12), (318, 13), (321, 9), (321, 10), (321, 11), (321, 12), (321, 13), (357, 9), (357, 10), (357, 11), (357, 12), (357, 13), (373, 9), (373, 10), (373, 11), (373, 12), (373, 13), (380, 9), (380, 10), (380, 11), (380, 12), (380, 13), (387, 9), (387, 10), (387, 11), (387, 12), (387, 13), (397, 9), (397, 10), (397, 11), (397, 12), (397, 13), (422, 9), (422, 10), (422, 11), (422, 12), (422, 13), (424, 9), (424, 10), (424, 11), (424, 12), (424, 13), (427, 9), (427, 10), (427, 11), (427, 12), (427, 13), (431, 9), (431, 10), (431, 11), (431, 12), (431, 13), (434, 9), (434, 10), (434, 11), (434, 12), (434, 13), (441, 9), (441, 10), (441, 11), (441, 12), (441, 13), (451, 9), (451, 10), (451, 11), (451, 12), (451, 13), (458, 9), (458, 10), (458, 11), (458, 12), (458, 13), (489, 9), (489, 10), (489, 11), (489, 12), (489, 13), (494, 9), (494, 10), (494, 11), (494, 12), (494, 13), (495, 9), (495, 10), (495, 11), (495, 12), (495, 13), (496, 9), (496, 10), (496, 11), (496, 12), (496, 13), (510, 9), (510, 10), (510, 11), (510, 12), (510, 13), (512, 9), (512, 10), (512, 11), (512, 12), (512, 13), (517, 9), (517, 10), (517, 11), (517, 12), (517, 13), (519, 9), (519, 10), (519, 11), (519, 12), (519, 13), (532, 9), (532, 10), (532, 11), (532, 12), (532, 13), (540, 9), (540, 10), (540, 11), (540, 12), (540, 13), (548, 9), (548, 10), (548, 11), (548, 12), (548, 13), (555, 9), (555, 10), (555, 11), (555, 12), (555, 13), (560, 9), (560, 10), (560, 11), (560, 12), (560, 13), (565, 9), (565, 10), (565, 11), (565, 12), (565, 13), (577, 9), (577, 10), (577, 11), (577, 12), (577, 13), (595, 9), (595, 10), (595, 11), (595, 12), (595, 13), (604, 9), (604, 10), (604, 11), (604, 12), (604, 13), (605, 9), (605, 10), (605, 11), (605, 12), (605, 13), (636, 9), (636, 10), (636, 11), (636, 12), (636, 13), (641, 9), (641, 10), (641, 11), (641, 12), (641, 13), (647, 9), (647, 10), (647, 11), (647, 12), (647, 13), (656, 9), (656, 10), (656, 11), (656, 12), (656, 13), (660, 9), (660, 10), (660, 11), (660, 12), (660, 13), (661, 9), (661, 10), (661, 11), (661, 12), (661, 13), (666, 9), (666, 10), (666, 11), (666, 12), (666, 13), (681, 9), (681, 10), (681, 11), (681, 12), (681, 13), (689, 9), (689, 10), (689, 11), (689, 12), (689, 13), (691, 9), (691, 10), (691, 11), (691, 12), (691, 13), (694, 9), (694, 10), (694, 11), (694, 12), (694, 13), (700, 9), (700, 10), (700, 11), (700, 12), (700, 13), (705, 9), (705, 10), (705, 11), (705, 12), (705, 13), (707, 9), (707, 10), (707, 11), (707, 12), (707, 13), (719, 9), (719, 10), (719, 11), (719, 12), (719, 13), (721, 9), (721, 10), (721, 11), (721, 12), (721, 13), (732, 9), (732, 10), (732, 11), (732, 12), (732, 13), (738, 9), (738, 10), (738, 11), (738, 12), (738, 13), (741, 9), (741, 10), (741, 11), (741, 12), (741, 13), (760, 9), (760, 10), (760, 11), (760, 12), (760, 13), (766, 9), (766, 10), (766, 11), (766, 12), (766, 13), (772, 9), (772, 10), (772, 11), (772, 12), (772, 13), (775, 9), (775, 10), (775, 11), (775, 12), (775, 13), (776, 9), (776, 10), (776, 11), (776, 12), (776, 13), (800, 9), (800, 10), (800, 11), (800, 12), (800, 13), (802, 9), (802, 10), (802, 11), (802, 12), (802, 13), (815, 9), (815, 10), (815, 11), (815, 12), (815, 13), (825, 9), (825, 10), (825, 11), (825, 12), (825, 13), (830, 9), (830, 10), (830, 11), (830, 12), (830, 13), (831, 9), (831, 10), (831, 11), (831, 12), (831, 13), (842, 9), (842, 10), (842, 11), (842, 12), (842, 13), (844, 9), (844, 10), (844, 11), (844, 12), (844, 13), (845, 9), (845, 10), (845, 11), (845, 12), (845, 13), (865, 9), (865, 10), (865, 11), (865, 12), (865, 13), (868, 9), (868, 10), (868, 11), (868, 12), (868, 13), (869, 9), (869, 10), (869, 11), (869, 12), (869, 13), (872, 9), (872, 10), (872, 11), (872, 12), (872, 13), (877, 9), (877, 10), (877, 11), (877, 12), (877, 13), (890, 9), (890, 10), (890, 11), (890, 12), (890, 13), (899, 9), (899, 10), (899, 11), (899, 12), (899, 13), (900, 9), (900, 10), (900, 11), (900, 12), (900, 13), (915, 9), (915, 10), (915, 11), (915, 12), (915, 13), (930, 9), (930, 10), (930, 11), (930, 12), (930, 13), (933, 9), (933, 10), (933, 11), (933, 12), (933, 13), (938, 9), (938, 10), (938, 11), (938, 12), (938, 13), (944, 9), (944, 10), (944, 11), (944, 12), (944, 13), (949, 9), (949, 10), (949, 11), (949, 12), (949, 13), (957, 9), (957, 10), (957, 11), (957, 12), (957, 13), (969, 9), (969, 10), (969, 11), (969, 12), (969, 13), (970, 9), (970, 10), (970, 11), (970, 12), (970, 13), (975, 9), (975, 10), (975, 11), (975, 12), (975, 13), (995, 9), (995, 10), (995, 11), (995, 12), (995, 13), (998, 9), (998, 10), (998, 11), (998, 12), (998, 13), (1000, 9), (1000, 10), (1000, 11), (1000, 12), (1000, 13), (1010, 9), (1010, 10), (1010, 11), (1010, 12), (1010, 13), (1012, 9), (1012, 10), (1012, 11), (1012, 12), (1012, 13), (1015, 9), (1015, 10), (1015, 11), (1015, 12), (1015, 13), (1018, 9), (1018, 10), (1018, 11), (1018, 12), (1018, 13), (1071, 9), (1071, 10), (1071, 11), (1071, 12), (1071, 13), (1073, 9), (1073, 10), (1073, 11), (1073, 12), (1073, 13), (1080, 9), (1080, 10), (1080, 11), (1080, 12), (1080, 13), (1102, 9), (1102, 10), (1102, 11), (1102, 12), (1102, 13), (1108, 9), (1108, 10), (1108, 11), (1108, 12), (1108, 13), (1114, 9), (1114, 10), (1114, 11), (1114, 12), (1114, 13), (1121, 9), (1121, 10), (1121, 11), (1121, 12), (1121, 13), (1129, 9), (1129, 10), (1129, 11), (1129, 12), (1129, 13), (1136, 9), (1136, 10), (1136, 11), (1136, 12), (1136, 13), (1145, 9), (1145, 10), (1145, 11), (1145, 12), (1145, 13), (1146, 9), (1146, 10), (1146, 11), (1146, 12), (1146, 13), (1172, 9), (1172, 10), (1172, 11), (1172, 12), (1172, 13), (1186, 9), (1186, 10), (1186, 11), (1186, 12), (1186, 13), (1193, 9), (1193, 10), (1193, 11), (1193, 12), (1193, 13), (1196, 9), (1196, 10), (1196, 11), (1196, 12), (1196, 13), (1203, 9), (1203, 10), (1203, 11), (1203, 12), (1203, 13), (1213, 9), (1213, 10), (1213, 11), (1213, 12), (1213, 13), (1216, 9), (1216, 10), (1216, 11), (1216, 12), (1216, 13), (1222, 9), (1222, 10), (1222, 11), (1222, 12), (1222, 13), (1245, 9), (1245, 10), (1245, 11), (1245, 12), (1245, 13), (1272, 9), (1272, 10), (1272, 11), (1272, 12), (1272, 13), (1273, 9), (1273, 10), (1273, 11), (1273, 12), (1273, 13), (1281, 9), (1281, 10), (1281, 11), (1281, 12), (1281, 13), (1287, 9), (1287, 10), (1287, 11), (1287, 12), (1287, 13), (1288, 9), (1288, 10), (1288, 11), (1288, 12), (1288, 13), (1292, 9), (1292, 10), (1292, 11), (1292, 12), (1292, 13), (1295, 9), (1295, 10), (1295, 11), (1295, 12), (1295, 13), (1300, 9), (1300, 10), (1300, 11), (1300, 12), (1300, 13), (1301, 9), (1301, 10), (1301, 11), (1301, 12), (1301, 13), (1306, 9), (1306, 10), (1306, 11), (1306, 12), (1306, 13), (1309, 9), (1309, 10), (1309, 11), (1309, 12), (1309, 13), (1322, 9), (1322, 10), (1322, 11), (1322, 12), (1322, 13), (1340, 9), (1340, 10), (1340, 11), (1340, 12), (1340, 13), (1358, 9), (1358, 10), (1358, 11), (1358, 12), (1358, 13), (1364, 9), (1364, 10), (1364, 11), (1364, 12), (1364, 13), (1370, 9), (1370, 10), (1370, 11), (1370, 12), (1370, 13), (1374, 9), (1374, 10), (1374, 11), (1374, 12), (1374, 13), (1382, 9), (1382, 10), (1382, 11), (1382, 12), (1382, 13), (1388, 9), (1388, 10), (1388, 11), (1388, 12), (1388, 13), (1394, 9), (1394, 10), (1394, 11), (1394, 12), (1394, 13), (1396, 9), (1396, 10), (1396, 11), (1396, 12), (1396, 13), (1397, 9), (1397, 10), (1397, 11), (1397, 12), (1397, 13), (1407, 9), (1407, 10), (1407, 11), (1407, 12), (1407, 13), (1420, 9), (1420, 10), (1420, 11), (1420, 12), (1420, 13), (1461, 9), (1461, 10), (1461, 11), (1461, 12), (1461, 13), (1483, 9), (1483, 10), (1483, 11), (1483, 12), (1483, 13), (1488, 9), (1488, 10), (1488, 11), (1488, 12), (1488, 13), (1490, 9), (1490, 10), (1490, 11), (1490, 12), (1490, 13), (1492, 9), (1492, 10), (1492, 11), (1492, 12), (1492, 13), (1494, 9), (1494, 10), (1494, 11), (1494, 12), (1494, 13), (1514, 9), (1514, 10), (1514, 11), (1514, 12), (1514, 13), (1530, 9), (1530, 10), (1530, 11), (1530, 12), (1530, 13), (1537, 9), (1537, 10), (1537, 11), (1537, 12), (1537, 13), (1539, 9), (1539, 10), (1539, 11), (1539, 12), (1539, 13), (1545, 9), (1545, 10), (1545, 11), (1545, 12), (1545, 13), (1548, 9), (1548, 10), (1548, 11), (1548, 12), (1548, 13), (1555, 9), (1555, 10), (1555, 11), (1555, 12), (1555, 13), (1564, 9), (1564, 10), (1564, 11), (1564, 12), (1564, 13), (1568, 9), (1568, 10), (1568, 11), (1568, 12), (1568, 13), (1577, 9), (1577, 10), (1577, 11), (1577, 12), (1577, 13), (1580, 9), (1580, 10), (1580, 11), (1580, 12), (1580, 13), (1592, 9), (1592, 10), (1592, 11), (1592, 12), (1592, 13), (1612, 9), (1612, 10), (1612, 11), (1612, 12), (1612, 13), (1626, 9), (1626, 10), (1626, 11), (1626, 12), (1626, 13), (1634, 9), (1634, 10), (1634, 11), (1634, 12), (1634, 13), (1646, 9), (1646, 10), (1646, 11), (1646, 12), (1646, 13), (1656, 9), (1656, 10), (1656, 11), (1656, 12), (1656, 13), (1658, 9), (1658, 10), (1658, 11), (1658, 12), (1658, 13), (1662, 9), (1662, 10), (1662, 11), (1662, 12), (1662, 13), (1667, 9), (1667, 10), (1667, 11), (1667, 12), (1667, 13), (1668, 9), (1668, 10), (1668, 11), (1668, 12), (1668, 13), (1670, 9), (1670, 10), (1670, 11), (1670, 12), (1670, 13), (1679, 9), (1679, 10), (1679, 11), (1679, 12), (1679, 13), (1689, 9), (1689, 10), (1689, 11), (1689, 12), (1689, 13), (1692, 9), (1692, 10), (1692, 11), (1692, 12), (1692, 13), (1694, 9), (1694, 10), (1694, 11), (1694, 12), (1694, 13), (1705, 9), (1705, 10), (1705, 11), (1705, 12), (1705, 13), (1706, 9), (1706, 10), (1706, 11), (1706, 12), (1706, 13), (1708, 9), (1708, 10), (1708, 11), (1708, 12), (1708, 13), (1712, 9), (1712, 10), (1712, 11), (1712, 12), (1712, 13), (1722, 9), (1722, 10), (1722, 11), (1722, 12), (1722, 13), (1739, 9), (1739, 10), (1739, 11), (1739, 12), (1739, 13), (1740, 9), (1740, 10), (1740, 11), (1740, 12), (1740, 13), (1745, 9), (1745, 10), (1745, 11), (1745, 12), (1745, 13), (1748, 9), (1748, 10), (1748, 11), (1748, 12), (1748, 13), (1750, 9), (1750, 10), (1750, 11), (1750, 12), (1750, 13), (1756, 9), (1756, 10), (1756, 11), (1756, 12), (1756, 13), (1767, 9), (1767, 10), (1767, 11), (1767, 12), (1767, 13), (1771, 9), (1771, 10), (1771, 11), (1771, 12), (1771, 13), (1806, 9), (1806, 10), (1806, 11), (1806, 12), (1806, 13), (1815, 9), (1815, 10), (1815, 11), (1815, 12), (1815, 13), (1824, 9), (1824, 10), (1824, 11), (1824, 12), (1824, 13), (1831, 9), (1831, 10), (1831, 11), (1831, 12), (1831, 13), (1835, 9), (1835, 10), (1835, 11), (1835, 12), (1835, 13), (1841, 9), (1841, 10), (1841, 11), (1841, 12), (1841, 13), (1842, 9), (1842, 10), (1842, 11), (1842, 12), (1842, 13), (1860, 9), (1860, 10), (1860, 11), (1860, 12), (1860, 13), (1864, 9), (1864, 10), (1864, 11), (1864, 12), (1864, 13), (1866, 9), (1866, 10), (1866, 11), (1866, 12), (1866, 13), (1878, 9), (1878, 10), (1878, 11), (1878, 12), (1878, 13), (1883, 9), (1883, 10), (1883, 11), (1883, 12), (1883, 13), (1890, 9), (1890, 10), (1890, 11), (1890, 12), (1890, 13), (1892, 9), (1892, 10), (1892, 11), (1892, 12), (1892, 13), (1895, 9), (1895, 10), (1895, 11), (1895, 12), (1895, 13), (1896, 9), (1896, 10), (1896, 11), (1896, 12), (1896, 13), (1907, 9), (1907, 10), (1907, 11), (1907, 12), (1907, 13), (1909, 9), (1909, 10), (1909, 11), (1909, 12), (1909, 13), (1915, 9), (1915, 10), (1915, 11), (1915, 12), (1915, 13), (1917, 9), (1917, 10), (1917, 11), (1917, 12), (1917, 13), (1922, 9), (1922, 10), (1922, 11), (1922, 12), (1922, 13), (1927, 9), (1927, 10), (1927, 11), (1927, 12), (1927, 13), (1940, 9), (1940, 10), (1940, 11), (1940, 12), (1940, 13), (1944, 9), (1944, 10), (1944, 11), (1944, 12), (1944, 13), (1947, 9), (1947, 10), (1947, 11), (1947, 12), (1947, 13), (1949, 9), (1949, 10), (1949, 11), (1949, 12), (1949, 13), (1960, 9), (1960, 10), (1960, 11), (1960, 12), (1960, 13), (1962, 9), (1962, 10), (1962, 11), (1962, 12), (1962, 13), (1963, 9), (1963, 10), (1963, 11), (1963, 12), (1963, 13), (1973, 9), (1973, 10), (1973, 11), (1973, 12), (1973, 13), (1989, 9), (1989, 10), (1989, 11), (1989, 12), (1989, 13), (1990, 9), (1990, 10), (1990, 11), (1990, 12), (1990, 13), (1995, 9), (1995, 10), (1995, 11), (1995, 12), (1995, 13)]\n",
      "訓練 RandomForestClassifier 模型，樣本數: 8012。\n",
      "訓練時間 3.4638 秒\n",
      "預測時間 in 0.0760 秒\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 41\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput size is :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_size))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mrecordMLoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_muti_modal_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m#     print(\"ERROR!!!\")\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# cal output size\u001b[39;00m\n\u001b[0;32m     49\u001b[0m linear_res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(model_name)\n",
      "Cell \u001b[1;32mIn[45], line 26\u001b[0m, in \u001b[0;36mrecordMLoutput\u001b[1;34m(ML_model, model, train_dataloader, test_dataloader, enable_muti_module)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndices of NaN values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(nan_indices[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;250m \u001b[39mnan_indices[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(len(test_features[0]))\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mML_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======eval finish!=========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f1, acc\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\XgbHelperFunction.py:21\u001b[0m, in \u001b[0;36mtrain_predict\u001b[1;34m(clf, X_train, y_train, X_test, y_test, evalByMMSE)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 在訓練集上評估模型\u001b[39;00m\n\u001b[0;32m     20\u001b[0m res1 , res2 \u001b[38;5;241m=\u001b[39m predict_labels(clf, X_train, y_train, evalByMMSE)\n\u001b[1;32m---> 21\u001b[0m res3, res4 \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalByMMSE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evalByMMSE :\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練集上的 MAE,RMSE : \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m , \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;124m。\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(res1, res2))\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\XgbHelperFunction.py:84\u001b[0m, in \u001b[0;36mpredict_labels\u001b[1;34m(clf, features, target, evalByMMSE)\u001b[0m\n\u001b[0;32m     74\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(features)  \u001b[38;5;66;03m# y_pred 會return probability\u001b[39;00m\n\u001b[0;32m     75\u001b[0m   \u001b[38;5;66;03m# prob_target = []\u001b[39;00m\n\u001b[0;32m     76\u001b[0m   \u001b[38;5;66;03m# for vec in target:\u001b[39;00m\n\u001b[0;32m     77\u001b[0m   \u001b[38;5;66;03m#   prob_target += [fromPermutationToProbability(vec)]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;66;03m# target = oneHotVecSerial(len(target))\u001b[39;00m\n\u001b[0;32m     82\u001b[0m   \u001b[38;5;66;03m# return mean_absolute_error(prob_target, prob_pred) , mean_squared_error(prob_target, prob_pred, squared=False)\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m預測時間 in \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:823\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;124;03m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    825\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39margmax(proba, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:865\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    863\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    868\u001b[0m n_jobs, _, _ \u001b[38;5;241m=\u001b[39m _partition_estimators(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:599\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    598\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 599\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X) \u001b[38;5;129;01mand\u001b[39;00m (X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc \u001b[38;5;129;01mor\u001b[39;00m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39mintc):\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB105\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"ERROR!!!\")\n",
    "            break\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # try:\n",
    "        f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=enable_muti_modal_signal)\n",
    "        # except:\n",
    "        #     print(\"ERROR!!!\")\n",
    "        #     break\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    \n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" +  model_select_signal + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "\n",
    "# asyncio.run(discord_bot_notifier(\"ML model訓練完成!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
