{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### control line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# model_select_signal = 'resnet18'\n",
    "model_select_signal = 'vgg16'\n",
    "\n",
    "data_select_signal = 'skin'\n",
    "# data_select_signal = 'chest CT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "elif data_select_signal == 'chest CT':\n",
    "    dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.returnDataloaders()\n",
    "train_files , test_files = dataContainer.returnDatasetFilenames()\n",
    "feature_vector_file_path = dataContainer.returnFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB\n",
    "import xgboost as xgb\n",
    "\n",
    " \n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "  if enable_muti_module:\n",
    "  #   for idx , (data, label) in enumerate(train_dataloader):\n",
    "  #     train_features[idx] = np.concatenate(train_features[idx] , calImgFeatureVector(data))\n",
    "  #   for idx , (data, label) in enumerate(test_dataloader):\n",
    "  #     test_features[idx] = np.concatenate(test_features[idx] , calImgFeatureVector(data))\n",
    "\n",
    "    for idx,feature in enumerate(test_features):\n",
    "        feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "    for idx,feature in enumerate(train_features):\n",
    "        feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n",
    "\n",
    "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  iter ,f1 ,acc = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, enable_f1_metric=True)\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc, iter\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "  if enable_muti_module:\n",
    "    for idx,feature in enumerate(test_features):\n",
    "        feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "    for idx,feature in enumerate(train_features):\n",
    "        feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n",
    "\n",
    "\n",
    "\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 7]                    --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 7]                    --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 7]                    --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-7                  [1, 7]                    28,679\n",
       "==========================================================================================\n",
       "Total params: 134,289,223\n",
       "Trainable params: 134,289,223\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.16\n",
       "Estimated Total Size (MB): 646.20\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "if data_select_signal == 'skin':\n",
    "    model_folder_path = \"../model/HAM10000\"\n",
    "elif data_select_signal == 'chest CT':\n",
    "    model_folder_path = \"../model/CT chest\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "summary(vgg16, [1,3,224,224])\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "\n",
    "if model_select_signal == 'resnet18':\n",
    "    model_0 = resnet18\n",
    "elif model_select_signal == 'vgg16':\n",
    "    model_0 = vgg16\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :8012\n",
      "8012\n",
      "<class 'numpy.ndarray'>\n",
      "[ 2.3175943   0.92064023  0.68780285  1.7439535  -1.1743599  -1.2689987\n",
      " -1.984735  ]\n"
     ]
    }
   ],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "print(len(train_features))\n",
    "print(type(train_features))\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 初始化层计数器\n",
    "# total_layers = 0\n",
    "\n",
    "# # 遍历模型的所有子模块和层\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # 打印每一层的名称和它的具体类型，这一步是可选的，但对理解模型结构很有帮助\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "#     # 对所有层进行计数（包括卷积层、全连接层等）\n",
    "#     # 如果只想计算特定类型的层（如卷积层Conv2d），则需要添加判断条件\n",
    "#     total_layers += 1\n",
    "\n",
    "# # 打印总层数\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# # 示例：仅计算Conv2d层的数量\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 44層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.968170813624821, 0.6598266499582288, 0.6505298027279679)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.helperFunctions import calDetailModelLayersNum\n",
    "\n",
    "\n",
    "# summary(list_of_models[23][0], input_size=[1,3,224,224],depth=4)\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0)):\n",
    "#     try : \n",
    "#         summary(list_of_models[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 512, 7, 7]            --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-1                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    └─BasicBlock: 2-2                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-9                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-10                 [1, 64, 56, 56]           36,864\n",
       "│    │    └─BatchNorm2d: 3-11            [1, 64, 56, 56]           128\n",
       "│    │    └─ReLU: 3-12                   [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-6                        [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-3                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-13                 [1, 128, 28, 28]          73,728\n",
       "│    │    └─BatchNorm2d: 3-14            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-15                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-16                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-17            [1, 128, 28, 28]          256\n",
       "│    │    └─Sequential: 3-18             [1, 128, 28, 28]          8,448\n",
       "│    │    └─ReLU: 3-19                   [1, 128, 28, 28]          --\n",
       "│    └─BasicBlock: 2-4                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-22                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-23                 [1, 128, 28, 28]          147,456\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 128, 28, 28]          256\n",
       "│    │    └─ReLU: 3-25                   [1, 128, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-5                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 14, 14]          294,912\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 256, 14, 14]          512\n",
       "│    │    └─Sequential: 3-31             [1, 256, 14, 14]          33,280\n",
       "│    │    └─ReLU: 3-32                   [1, 256, 14, 14]          --\n",
       "│    └─BasicBlock: 2-6                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-33                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-34            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-35                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-36                 [1, 256, 14, 14]          589,824\n",
       "│    │    └─BatchNorm2d: 3-37            [1, 256, 14, 14]          512\n",
       "│    │    └─ReLU: 3-38                   [1, 256, 14, 14]          --\n",
       "├─Sequential: 1-8                        [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-7                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-39                 [1, 512, 7, 7]            1,179,648\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-41                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-42                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 512, 7, 7]            1,024\n",
       "│    │    └─Sequential: 3-44             [1, 512, 7, 7]            132,096\n",
       "│    │    └─ReLU: 3-45                   [1, 512, 7, 7]            --\n",
       "│    └─BasicBlock: 2-8                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-46                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-47            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-48                   [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-49                 [1, 512, 7, 7]            2,359,296\n",
       "│    │    └─BatchNorm2d: 3-50            [1, 512, 7, 7]            1,024\n",
       "│    │    └─ReLU: 3-51                   [1, 512, 7, 7]            --\n",
       "==========================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.81\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 85.05\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[2][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 32\n",
      "======eval finish!=========\n",
      "best acc is : -0.718922\n",
      "best f1 is : -0.689144\n",
      "model_name : layer:44\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 50\n",
      "======eval finish!=========\n",
      "best acc is : -0.787319\n",
      "best f1 is : -0.764532\n",
      "model_name : layer:43\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 50\n",
      "======eval finish!=========\n",
      "best acc is : -0.787319\n",
      "best f1 is : -0.764532\n",
      "model_name : layer:42\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 38\n",
      "======eval finish!=========\n",
      "best acc is : -0.784823\n",
      "best f1 is : -0.764504\n",
      "model_name : layer:41\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 64\n",
      "======eval finish!=========\n",
      "best acc is : -0.787319\n",
      "best f1 is : -0.764794\n",
      "model_name : layer:40\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 64\n",
      "======eval finish!=========\n",
      "best acc is : -0.787319\n",
      "best f1 is : -0.764794\n",
      "model_name : layer:39\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 80\n",
      "======eval finish!=========\n",
      "best acc is : -0.789316\n",
      "best f1 is : -0.767589\n",
      "model_name : layer:38\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 100\n",
      "======eval finish!=========\n",
      "best acc is : -0.783325\n",
      "best f1 is : -0.75666\n",
      "model_name : layer:37\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 100\n",
      "======eval finish!=========\n",
      "best acc is : -0.783325\n",
      "best f1 is : -0.75666\n",
      "model_name : layer:36\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 100\n",
      "======eval finish!=========\n",
      "best acc is : -0.783325\n",
      "best f1 is : -0.75666\n",
      "model_name : layer:35\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 100\n",
      "======eval finish!=========\n",
      "best acc is : -0.783325\n",
      "best f1 is : -0.75666\n",
      "model_name : layer:34\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n",
      "Best iteration: 100\n",
      "======eval finish!=========\n",
      "best acc is : -0.783325\n",
      "best f1 is : -0.75666\n",
      "model_name : layer:33\n",
      "input size is :[1, 512, 14, 14]\n",
      "cal CNN model output...\n",
      "feature size is :2003\n",
      "feature size is :8012\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[20:08:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:232: bad_malloc: Failed to allocate 13317808704 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput size is :\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(input_size))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     f1, acc, \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mrecordXGBoutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43menable_muti_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough memory!! escape form loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36mrecordXGBoutput\u001b[1;34m(model, train_dataloader, test_dataloader, enable_muti_module)\u001b[0m\n\u001b[0;32m     21\u001b[0m       feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((feature , np\u001b[38;5;241m.\u001b[39marray(train_feature_vectors[idx])))\n\u001b[0;32m     23\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m, num_class\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28miter\u001b[39m ,f1 ,acc \u001b[38;5;241m=\u001b[39m \u001b[43mcalBestIterOfXGB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_f1_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m======eval finish!=========\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\deepXGB\\deepXGB\\program\\helperFunction\\XgbHelperFunction.py:131\u001b[0m, in \u001b[0;36mcalBestIterOfXGB\u001b[1;34m(train_features, train_labels, test_features, test_labels, enable_f1_metric)\u001b[0m\n\u001b[0;32m    126\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softprob\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: num_class}\n\u001b[0;32m    130\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 131\u001b[0m bst \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m105\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m                \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest iteration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbst\u001b[38;5;241m.\u001b[39mbest_iteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enable_f1_metric:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2050\u001b[0m     \u001b[43m_check_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2052\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2056\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:282\u001b[0m, in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    return value from API calls\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[38;5;241m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [20:08:57] C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\common\\io.h:232: bad_malloc: Failed to allocate 13317808704 bytes."
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[]}\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "    try:\n",
    "        f1, acc, iter = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=False)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    print(\"best acc is : {}\".format(acc[iter]))\n",
    "    print(\"best f1 is : {}\".format(f1[iter]))\n",
    "    print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    cur_acc = acc[iter]\n",
    "    XGB_res[\"acc\"].append((-cur_acc) if cur_acc < 0 else cur_acc)\n",
    "    cur_f1 = f1[iter]\n",
    "    XGB_res[\"f1\"].append((-cur_f1) if cur_f1 < 0 else cur_f1)\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 12:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 4]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n",
      "訓練時間 0.0750 秒\n",
      "預測時間 in 0.0010 秒\n",
      "預測時間 in 0.0000 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9967 , 0.9967。\n",
      "測試集的 F1 score和acc分別為: 0.8164 , 0.8159。\n",
      "======eval finish!=========\n",
      "layer:68\n",
      "input size is :[1, 512, 1, 1]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n",
      "訓練時間 0.1623 秒\n",
      "預測時間 in 0.0020 秒\n",
      "預測時間 in 0.0010 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7999 , 0.8000。\n",
      "======eval finish!=========\n",
      "layer:67\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 6.1888 秒\n",
      "預測時間 in 0.0530 秒\n",
      "預測時間 in 0.0280 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7907 , 0.7905。\n",
      "======eval finish!=========\n",
      "layer:66\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n",
      "訓練時間 3.7406 秒\n",
      "預測時間 in 0.0530 秒\n",
      "預測時間 in 0.0340 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8018 , 0.8032。\n",
      "======eval finish!=========\n",
      "layer:65\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 6.0211 秒\n",
      "預測時間 in 0.0530 秒\n",
      "預測時間 in 0.0270 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.7967 , 0.7968。\n",
      "======eval finish!=========\n",
      "layer:64\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 4.9186 秒\n",
      "預測時間 in 0.0600 秒\n",
      "預測時間 in 0.0280 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.8354 , 0.8349。\n",
      "======eval finish!=========\n",
      "layer:63\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 5.0312 秒\n",
      "預測時間 in 0.1040 秒\n",
      "預測時間 in 0.0520 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7954 , 0.7968。\n",
      "======eval finish!=========\n",
      "layer:62\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 5.0159 秒\n",
      "預測時間 in 0.0970 秒\n",
      "預測時間 in 0.0330 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8063 , 0.8063。\n",
      "======eval finish!=========\n",
      "layer:61\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 4.7551 秒\n",
      "預測時間 in 0.0940 秒\n",
      "預測時間 in 0.0510 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7969 , 0.7968。\n",
      "======eval finish!=========\n",
      "layer:60\n",
      "MAT ERROR!!!\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 4.9170 秒\n",
      "預測時間 in 0.1070 秒\n",
      "預測時間 in 0.0540 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7980 , 0.7968。\n",
      "======eval finish!=========\n",
      "layer:58\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 4.9710 秒\n",
      "預測時間 in 0.0940 秒\n",
      "預測時間 in 0.0460 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8094 , 0.8095。\n",
      "======eval finish!=========\n",
      "layer:57\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 5.1324 秒\n",
      "預測時間 in 0.0593 秒\n",
      "預測時間 in 0.0391 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8191 , 0.8190。\n",
      "======eval finish!=========\n",
      "layer:56\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 4.8932 秒\n",
      "預測時間 in 0.0530 秒\n",
      "預測時間 in 0.0280 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.7629 , 0.7619。\n",
      "======eval finish!=========\n",
      "layer:55\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練時間 5.1591 秒\n",
      "預測時間 in 0.0662 秒\n",
      "預測時間 in 0.0420 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.7716 , 0.7746。\n",
      "======eval finish!=========\n",
      "layer:54\n",
      "input size is :[1, 512, 8, 8]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 LogisticRegression 模型，樣本數: 613。\n",
      "訓練時間 4.9958 秒\n",
      "預測時間 in 0.0600 秒\n",
      "預測時間 in 0.0280 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.8057 , 0.8063。\n",
      "======eval finish!=========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear_model_results_resnet18.csv'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"MAT ERROR!!!\")\n",
    "            continue\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=False)\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    \n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:62_model_results_resnet18.csv'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25088])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test\n",
    "from helperFunction.helperFunctions import flattenExceptDim0\n",
    "output = flattenExceptDim0(torch.rand([32,512,7,7]))\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate nn output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50251264,)\n",
      "(2003,)\n"
     ]
    }
   ],
   "source": [
    "print(test_features.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add features (LBP, csv...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_features):\n\u001b[1;32m----> 2\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_feature_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_features):\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((feature , np\u001b[38;5;241m.\u001b[39marray(train_feature_vectors[idx])))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "for idx,feature in enumerate(test_features):\n",
    "    feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "for idx,feature in enumerate(train_features):\n",
    "    feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練 XGBClassifier 模型，樣本數: 201005056。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please reshape the input data into 2-dimensional matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m, num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# xgb_model.fit(features, labels)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\skin_cancer_dataset\\program\\helperFunction\\XgbHelperFunction.py:18\u001b[0m, in \u001b[0;36mtrain_predict\u001b[1;34m(clf, X_train, y_train, X_test, y_test, evalByMMSE)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m 模型，樣本數: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m。\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(clf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train)))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 在訓練集上評估模型\u001b[39;00m\n\u001b[0;32m     20\u001b[0m res1 , res2 \u001b[38;5;241m=\u001b[39m predict_labels(clf, X_train, y_train, evalByMMSE)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\skin_cancer_dataset\\program\\helperFunction\\XgbHelperFunction.py:65\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(clf, X_train, y_train)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 紀錄訓練時間\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(\"訓練資料 : \".format(X_train[0:4]))\u001b[39;00m\n\u001b[0;32m     64\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練時間 \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1500\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1489\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m   1491\u001b[0m (\n\u001b[0;32m   1492\u001b[0m     model,\n\u001b[0;32m   1493\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1499\u001b[0m )\n\u001b[1;32m-> 1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1520\u001b[0m     params,\n\u001b[0;32m   1521\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1531\u001b[0m )\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:521\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    502\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    503\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    519\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:958\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1529\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1510\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m         )\n\u001b[0;32m   1523\u001b[0m     ):\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1525\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1526\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1527\u001b[0m         )\n\u001b[1;32m-> 1529\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1588\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1576\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1577\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1578\u001b[0m )\n\u001b[0;32m   1579\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1587\u001b[0m )\n\u001b[1;32m-> 1588\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:576\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:557\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:641\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n\u001b[0;32m    640\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1280\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1280\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:632\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m--> 632\u001b[0m \u001b[43mdispatch_proxy_set_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allow_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mset_info(\n\u001b[0;32m    634\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m    635\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    637\u001b[0m )\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1331\u001b[0m, in \u001b[0;36mdispatch_proxy_set_data\u001b[1;34m(proxy, data, cat_codes, allow_host)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch for QuantileDMatrix.\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_cudf_ser(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m-> 1331\u001b[0m     \u001b[43m_check_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data):\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;66;03m# pylint: disable=W0212\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m     proxy\u001b[38;5;241m.\u001b[39m_set_data_from_cuda_columnar(data, cast(List, cat_codes))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:57\u001b[0m, in \u001b[0;36m_check_data_shape\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_shape\u001b[39m(data: DataType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease reshape the input data into 2-dimensional matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Please reshape the input data into 2-dimensional matrix."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from helperFunction.XgbHelperFunction import train_predict\n",
    "\n",
    "# 創建XGBoost分類器\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=7)\n",
    "\n",
    "# 訓練模型\n",
    "f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "# xgb_model.fit(features, labels)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more detail XGB info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.23198\ttrain-Accuracy:0.87132\ttrain-F1-score:0.86772\tval-mlogloss:1.25322\tval-Accuracy:0.83974\tval-F1-score:0.83289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-mlogloss:0.94329\ttrain-Accuracy:0.87881\ttrain-F1-score:0.87539\tval-mlogloss:0.98333\tval-Accuracy:0.84373\tval-F1-score:0.83783\n",
      "[2]\ttrain-mlogloss:0.76241\ttrain-Accuracy:0.88118\ttrain-F1-score:0.87782\tval-mlogloss:0.81834\tval-Accuracy:0.84473\tval-F1-score:0.83857\n",
      "[3]\ttrain-mlogloss:0.63912\ttrain-Accuracy:0.88280\ttrain-F1-score:0.87919\tval-mlogloss:0.70796\tval-Accuracy:0.84323\tval-F1-score:0.83756\n",
      "[4]\ttrain-mlogloss:0.54987\ttrain-Accuracy:0.88530\ttrain-F1-score:0.88217\tval-mlogloss:0.62823\tval-Accuracy:0.84523\tval-F1-score:0.83996\n",
      "[5]\ttrain-mlogloss:0.48412\ttrain-Accuracy:0.88892\ttrain-F1-score:0.88588\tval-mlogloss:0.57188\tval-Accuracy:0.84523\tval-F1-score:0.83965\n",
      "[6]\ttrain-mlogloss:0.43472\ttrain-Accuracy:0.88954\ttrain-F1-score:0.88653\tval-mlogloss:0.53085\tval-Accuracy:0.84623\tval-F1-score:0.84082\n",
      "[7]\ttrain-mlogloss:0.39629\ttrain-Accuracy:0.89179\ttrain-F1-score:0.88892\tval-mlogloss:0.50091\tval-Accuracy:0.84573\tval-F1-score:0.84014\n",
      "[8]\ttrain-mlogloss:0.36640\ttrain-Accuracy:0.89328\ttrain-F1-score:0.89043\tval-mlogloss:0.47813\tval-Accuracy:0.84972\tval-F1-score:0.84447\n",
      "[9]\ttrain-mlogloss:0.34207\ttrain-Accuracy:0.89578\ttrain-F1-score:0.89311\tval-mlogloss:0.46274\tval-Accuracy:0.84723\tval-F1-score:0.84180\n",
      "Best iteration: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    # 将预测结果转换为类别标签\n",
    "    mlogloss = log_loss(labels, preds, labels=np.unique(dtrain.get_label()))\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # 用 'weighted' 适应类别不平衡\n",
    "    return [('Accuracy', acc), ('F1-score', f1), ('mlogloss', mlogloss)]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "dval = xgb.DMatrix(test_features, label=test_labels)\n",
    "\n",
    "# 设置参数，注意要更改 'objective' 为多分类的目标函数\n",
    "num_class = len(np.unique(train_labels))  # 获取类别数\n",
    "params = {'objective': 'multi:softprob','eval_metric': 'mlogloss', 'num_class': num_class}\n",
    "\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=105, \n",
    "                evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                custom_metric=custom_eval, evals_result=evals_result, \n",
    "                early_stopping_rounds=10,\n",
    "                verbose_eval=True)\n",
    "\n",
    "print(f\"Best iteration: {bst.best_iteration}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
