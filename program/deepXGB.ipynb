{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_file = '../data/img_conv2.csv'\n",
    "random_seed = 42\n",
    "# dataset_path = \"../data/HAM10000/images/\"\n",
    "# groundtruth_file = '../data/HAM10000/GroundTruth.csv'\n",
    "# feature_vector_file_path = '../data/HAM10000/img_feature_no_masked.csv'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groundtruth_data = pd.read_csv(groundtruth_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is DataFrame\n",
    "# df = pd.DataFrame(...)\n",
    "\n",
    "# def find_indices_of_ones(row):\n",
    "#     # 尋找前六個元素中 1 的位置\n",
    "#     return [(i-1) for i, x in enumerate(row[:8]) if x == 1]\n",
    "\n",
    "# # 將函數應用於每行並創建新列 'label'\n",
    "# groundtruth_data['label'] = groundtruth_data.apply(find_indices_of_ones, axis=1)\n",
    "\n",
    "# # groundtruth_data.head()\n",
    "\n",
    "# # find ele == 1\n",
    "# filename_to_label_dict = groundtruth_data.set_index('image')['label'].to_dict()\n",
    "\n",
    "# # {'1234' : [2] ,'1235' : [3] } -> {'1234' : 2 ,'1235' : 3 }\n",
    "# filename_to_label_dict =  {key: value[0] if value else None for key, value in filename_to_label_dict.items()}\n",
    "\n",
    "# type(filename_to_label_dict['ISIC_0024306'])\n",
    "\n",
    "# # {'1234' : 2 ,'1235' : 3 } -> {'1234.jpg' : 2 ,'1235.jpg' : 3 }\n",
    "# filename_to_label_dict =  {key + \".jpg\": value for key, value in filename_to_label_dict.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # check if value is out of range\n",
    "# null_keys = [key for key, value in filename_to_label_dict.items() if value is None]\n",
    "\n",
    "\n",
    "# all_values_in_range = all(0 <= value <= 6 for value in filename_to_label_dict.values())\n",
    "\n",
    "# # 輸出結果\n",
    "# print(\"所有的值都在範圍內：\" if all_values_in_range else \"有些值不在範圍內。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunction.CustomImageDataset import CustomImageDataset\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "# 數據轉換\n",
    "transform_std = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform224 = torchvision.transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "    # 其他轉換\n",
    "])\n",
    "\n",
    "transform = transform_std\n",
    "\n",
    "\n",
    "# all_files = [f for f in os.listdir(dataset_path) if os.path.isfile(os.path.join(dataset_path, f))]\n",
    "\n",
    "# random.seed(42)\n",
    "\n",
    "# num_total_samples = len(all_files)\n",
    "# split_ratio = 0.8  # 80% 的数据用于训练，20% 用于测试\n",
    "\n",
    "# # 随机打乱数据集\n",
    "# random.shuffle(all_files)\n",
    "\n",
    "# # 计算分割点\n",
    "# split_idx = int(num_total_samples * split_ratio)\n",
    "\n",
    "# # 分割数据集\n",
    "# train_files = all_files[:split_idx]\n",
    "# test_files = all_files[split_idx:]\n",
    "\n",
    "# 載入train & test file list\n",
    "# with open('test_files_list.json', 'r') as f:\n",
    "#      test_files = json.load(f)\n",
    "# with open('train_files_list.json', 'r') as f:\n",
    "#      train_files = json.load(f)\n",
    "\n",
    "\n",
    "# # 加載數據\n",
    "# train_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in train_files}, transform=transform)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# test_dataset = CustomImageDataset(img_dir=dataset_path,file_to_label_dict={file: filename_to_label_dict[file] for file in test_files}, transform=transform)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ transfrom test unit\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# class TestTransforms224Gray32bit(unittest.TestCase):\n",
    "#     def setUp(self, img_path=None):\n",
    "#         # 创建一个纯白色的测试图像，尺寸为 300x300\n",
    "#         if img_path :\n",
    "#             self.image = Image.open(img_path).convert('L')\n",
    "#         else :\n",
    "#             self.image = Image.new('L', (300, 300), color='white')\n",
    "\n",
    "#     def test_transform_flow(self):\n",
    "#         transform224_gray_32bit = transforms.Compose([\n",
    "#             transforms.Lambda(lambda img: np.array(img).astype(np.float32) / 255.0),\n",
    "#             transforms.Lambda(lambda x: torch.from_numpy(x)),\n",
    "#             transforms.Resize(256),\n",
    "#             transforms.CenterCrop(224),\n",
    "#             transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "#         ])\n",
    "        \n",
    "#         # 应用转换流程\n",
    "#         transformed_img = transform224_gray_32bit(self.image)\n",
    "        \n",
    "#         # 检查转换后的类型和形状\n",
    "#         self.assertTrue(isinstance(transformed_img, torch.Tensor), \"Output should be a Torch Tensor\")\n",
    "#         self.assertEqual(transformed_img.dtype, torch.float32, \"Output tensor should have dtype float32\")\n",
    "#         self.assertEqual(transformed_img.size(), (1, 224, 224), \"Output tensor should have shape (1, 224, 224)\")\n",
    "        \n",
    "#         # 检查值的范围是否合理（因为原图是纯白的，所以归一化后应该有一个固定的范围）\n",
    "#         expected_value = (1 - 0.485) / 0.229\n",
    "#         self.assertTrue(torch.allclose(transformed_img.mean(), torch.tensor(expected_value), atol=1e-5),\n",
    "#                         \"Normalized values do not match expected range\")\n",
    "\n",
    "# test = TestTransforms224Gray32bit()\n",
    "# test.setUp('../data/chestCTData/images/train/adenocarcinoma/000000 (6).png')\n",
    "\n",
    "# test.test_transform_flow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['adenocarcinoma', 'large.cell.carcinoma', 'normal', 'squamous.cell.carcinoma']\n",
      "Class to index: {'adenocarcinoma': 0, 'large.cell.carcinoma': 1, 'normal': 2, 'squamous.cell.carcinoma': 3}\n",
      "Samples: [('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000108 (3).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (2).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (4).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000109 (5).png', 0), ('c:\\\\Users\\\\E\\\\Desktop\\\\deepXGB\\\\deepXGB\\\\data\\\\chestCTData\\\\images\\\\test\\\\adenocarcinoma\\\\000112 (2).png', 0)]\n",
      "First image size: torch.Size([3, 224, 224])\n",
      "First image label: 0\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "\n",
    "from data.HAM10000.ham10000Dataloader import HAM10000DataProcessor\n",
    "from data.chestCTData.chestCTDataloader import ChestCTDataProcessor\n",
    "\n",
    "# dataContainer = HAM10000DataProcessor(transform=transform_std)\n",
    "dataContainer = ChestCTDataProcessor(transform=transform_std)\n",
    "\n",
    "train_dataloader , test_dataloader = dataContainer.returnDataloaders()\n",
    "train_files , test_files = dataContainer.returnDatasetFilenames()\n",
    "feature_vector_file_path = dataContainer.returnFeatureVectorFilename()\n",
    "\n",
    "num_classes = dataContainer.getNumClasses()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cal img feature vectors\n",
    "\n",
    "# from helperFunction.helperFunctions import calImgFeatureVector\n",
    "# print(train_dataloader)\n",
    "\n",
    "# test_feature_vectors = []\n",
    "# train_feature_vectors = []\n",
    "\n",
    "# for images, label in train_dataloader:\n",
    "#     for image in images:\n",
    "#         # image[0] for channel 0 image\n",
    "#         train_feature_vectors.append(calImgFeatureVector(image[0]))\n",
    "\n",
    "# for images, label in test_dataloader:\n",
    "#     for image in images:\n",
    "#         test_feature_vectors.append(calImgFeatureVector(image[0]))\n",
    "\n",
    "\n",
    "# print(len(train_feature_vectors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.transforms.transforms.Compose'>\n"
     ]
    }
   ],
   "source": [
    "print(type(transform224))\n",
    "\n",
    "# print(train_files[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load color feature data base on dataloader filename idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.XgbHelperFunction import csvkeylistToData\n",
    "\n",
    "\n",
    "\n",
    "# train_feature_vectors = csvkeylistToData(feature_vector_file_path, train_files)\n",
    "# test_feature_vectors = csvkeylistToData(feature_vector_file_path, test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define XGB eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define XGB training function\n",
    "from helperFunction.helperFunctions import dataloaderToFeatureData , calImgFeatureVector\n",
    "from helperFunction.XgbHelperFunction import  train_predict, calBestIterOfXGB\n",
    "import xgboost as xgb\n",
    "\n",
    " \n",
    "def recordXGBoutput(model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "  if enable_muti_module:\n",
    "  #   for idx , (data, label) in enumerate(train_dataloader):\n",
    "  #     train_features[idx] = np.concatenate(train_features[idx] , calImgFeatureVector(data))\n",
    "  #   for idx , (data, label) in enumerate(test_dataloader):\n",
    "  #     test_features[idx] = np.concatenate(test_features[idx] , calImgFeatureVector(data))\n",
    "\n",
    "    for idx,feature in enumerate(test_features):\n",
    "        feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "    for idx,feature in enumerate(train_features):\n",
    "        feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n",
    "\n",
    "  xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes)\n",
    "\n",
    "  iter ,f1 ,acc = calBestIterOfXGB(train_features, train_labels, test_features, test_labels, enable_f1_metric=True)\n",
    "\n",
    "  # f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc, iter\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define ML eval recorder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recordMLoutput(ML_model, model:nn.Sequential, train_dataloader, test_dataloader, enable_muti_module=False):\n",
    "  print(\"cal CNN model output...\")\n",
    "  test_features , test_labels = dataloaderToFeatureData(model, test_dataloader,device)\n",
    "  train_features , train_labels = dataloaderToFeatureData(model, train_dataloader, device)\n",
    "\n",
    "  if enable_muti_module:\n",
    "    for idx,feature in enumerate(test_features):\n",
    "        feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "    for idx,feature in enumerate(train_features):\n",
    "        feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n",
    "\n",
    "\n",
    "\n",
    "  f1, acc = train_predict(ML_model, train_features, train_labels,  test_features, test_labels)\n",
    "\n",
    "  print(\"======eval finish!=========\")\n",
    "\n",
    "  return f1, acc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init nn module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "VGG                                      [1, 4]                    --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 4]                    --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 4]                    --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-6                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-7                  [1, 4]                    16,388\n",
       "==========================================================================================\n",
       "Total params: 134,276,932\n",
       "Trainable params: 134,276,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.11\n",
       "Estimated Total Size (MB): 646.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# model_folder_path = \"../model/HAM10000\"\n",
    "model_folder_path = \"../model/CT chest\"\n",
    "\n",
    "\n",
    "\n",
    "# resnet101 = models.resnet101(pretrained=True)\n",
    "# #  ===================================\n",
    "# # 加載預訓練的ResNet模型\n",
    "# resnet18 = models.resnet18(pretrained=True)\n",
    "# resnet18 = torch.nn.Sequential(*(list(resnet18.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 加載訓練好的ResNet模型\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet18.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet18.pth\"))\n",
    "# resnet18_7  = torch.nn.Sequential(*(list(resnet18_7.children())[:-1]))  # 移除最後的全連接層\n",
    "\n",
    "# #  =========================\n",
    "# resnet50 = models.resnet50(pretrained=True)\n",
    "# num_ftrs = resnet50.fc.in_features\n",
    "# resnet50.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# resnet50.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_Resnet50_7.pth\"))\n",
    "\n",
    "# #  ===================================\n",
    "# # 加載預訓練的VGG模型\n",
    "# vgg16 = models.vgg16(pretrained=True)\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n",
    "\n",
    "#  ===================================\n",
    "# 載入訓練好的vgg\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "classifier = list(vgg16.classifier.children())[:-1]\n",
    "\n",
    "# 移除原始模型的最后一个全连接层\n",
    "# 并添加一个新的全连接层，输出特征数为 輸出的種類數\n",
    "classifier.append(torch.nn.Linear(4096, num_classes))\n",
    "\n",
    "# 替换原始模型的分类器\n",
    "vgg16.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "vgg16.load_state_dict(torch.load(model_folder_path + \"/best_model_pretrain_VGG16.pth\"))\n",
    "\n",
    "# 使用nn.Sequential的方式取代torch.flatten的功能\n",
    "new_classfier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    vgg16.classifier,\n",
    ")\n",
    "\n",
    "vgg16.classifier = new_classfier\n",
    "\n",
    "summary(vgg16, [1,3,224,224])\n",
    "# vgg16.classifier = torch.nn.Sequential(*list(vgg16.classifier.children())[:-1]) # 移除最後的全連接層\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ test unit : vgg16 modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定使用的模型\n",
    "\n",
    "# model_0 = resnet18\n",
    "model_0 = vgg16\n",
    "\n",
    "model_0 = model_0.to(device)\n",
    "\n",
    "summary(model_0, input_size=[1,3,224,224])\n",
    "\n",
    "# get var name\n",
    "model_0_name = [name for name, val in globals().items() if val == model_0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature size is :613\n",
      "613\n",
      "<class 'numpy.ndarray'>\n",
      "[ 3.998031   6.1575465 -2.9123302 -1.3426387]\n"
     ]
    }
   ],
   "source": [
    "# @ function test unit : dataloaderToFeatureData w/ img feature enhence\n",
    "train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n",
    "print(len(train_features))\n",
    "print(type(train_features))\n",
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : find specific layers\n",
    "\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model_0 = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 初始化层计数器\n",
    "# total_layers = 0\n",
    "\n",
    "# # 遍历模型的所有子模块和层\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     # 打印每一层的名称和它的具体类型，这一步是可选的，但对理解模型结构很有帮助\n",
    "#     # print(name, layer.__class__.__name__)\n",
    "\n",
    "#     # 对所有层进行计数（包括卷积层、全连接层等）\n",
    "#     # 如果只想计算特定类型的层（如卷积层Conv2d），则需要添加判断条件\n",
    "#     total_layers += 1\n",
    "\n",
    "# # 打印总层数\n",
    "# print(f'Total number of layers: {total_layers}')\n",
    "\n",
    "# # 示例：仅计算Conv2d层的数量\n",
    "# conv_layers = 0\n",
    "# for name, layer in model_0.named_modules():\n",
    "#     if isinstance(layer, torch.nn.Sequential):\n",
    "#         print(name, layer.__class__.__name__)\n",
    "#         conv_layers += 1\n",
    "\n",
    "# print(f'Total number of Sequential layers: {conv_layers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #  hook法，但會造成memory leak\n",
    "# import torch\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # 定义一个空字典来保存每一层的输出\n",
    "# layer_outputs = {}\n",
    "\n",
    "# # 定义一个钩子函数，它会在前向传播时被调用\n",
    "# def get_layer_output(module, input, output):\n",
    "#     # 将层的输出保存到字典中\n",
    "#     # 这里使用 module 的名称作为键，输出作为值\n",
    "#     layer_outputs[module.__class__.__name__] = output\n",
    "\n",
    "# # 加载预训练的ResNet18模型\n",
    "# model = models.resnet18(pretrained=True)\n",
    "\n",
    "# # 遍历模型中的所有模块，并为每个模块注册前向钩子\n",
    "# for name, module in model.named_modules():\n",
    "#     # 注册前向钩子\n",
    "#     module.register_forward_hook(get_layer_output)\n",
    "\n",
    "# # 准备一个输入张量\n",
    "# # 假设输入图片是3通道的224x224，这是ResNet18期望的输入维度\n",
    "# input_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# # 通过模型执行前向传播\n",
    "# # 这将触发我们之前注册的钩子，并收集每一层的输出\n",
    "# model(input_tensor)\n",
    "\n",
    "# # 现在layer_outputs字典中保存了模型中每一层的输出\n",
    "# # 你可以打印或检查这些输出\n",
    "# for layer_name, output in layer_outputs.items():\n",
    "#     print(f\"{layer_name}: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立輸出為不同隱藏層的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總層數為: 44層\n"
     ]
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import createDetailLayerVersions \n",
    "# detail version\n",
    "\n",
    "list_of_models = createDetailLayerVersions(model_0)\n",
    "\n",
    "# block level version\n",
    "\n",
    "# list_of_models = []\n",
    "\n",
    "# layer = 10\n",
    "# list_of_models.append((model_0 , \"layer:\"+str(layer)))\n",
    "# model = model_0\n",
    "\n",
    "# while layer > 0:\n",
    "#     model =  torch.nn.Sequential(*(list(model.children())[:-1])) \n",
    "#     layer -= 1\n",
    "#     list_of_models.append((model , \"layer:\"+str(layer)))\n",
    "\n",
    "# print(list_of_models)\n",
    "\n",
    "# len(list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : eval ability of model, in list of models\n",
    "# summary(model_0, input_size=[1,3,224,224])\n",
    "# summary(list_of_models[67][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[66][0] , input_size=[1,3,224,224])\n",
    "# summary(list_of_models[0][0] , input_size=[1,3,224,224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ unit test : dataloaerToFeatureData()\n",
    "\n",
    "# # model.forward( {data in datalaoder} ) -> (features , label)\n",
    "# from helperFunction.helperFunctions import dataloaderToFeatureData\n",
    "\n",
    "# model_0 = torch.nn.Sequential(*(list(model_0.children())[:-1])) \n",
    "\n",
    "# test_features , test_labels = dataloaderToFeatureData(model_0, test_dataloader,device)\n",
    "# train_features , train_labels = dataloaderToFeatureData(model_0, train_dataloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5948834598064423, 0.7689814814814815, 0.7671732646116883)\n"
     ]
    }
   ],
   "source": [
    "# @ model test : 確認載入的model性能與原本相符\n",
    "\n",
    "from helperFunction.TrainHelper import TrainingHelper\n",
    "\n",
    "train_helper = TrainingHelper(model_0,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              loss_fn=nn.CrossEntropyLoss(),\n",
    "                              optimizer=torch.optim.SGD(model_0.parameters(), lr = 0.001),\n",
    "                              device=device)\n",
    "\n",
    "print(train_helper.test_step())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helperFunction.helperFunctions import calDetailModelLayersNum\n",
    "\n",
    "\n",
    "# summary(list_of_models[23][0], input_size=[1,3,224,224],depth=4)\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0)):\n",
    "#     try : \n",
    "#         summary(list_of_models[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 4096]                 --\n",
       "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
       "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
       "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
       "│    └─Conv2d: 2-3                       [1, 64, 224, 224]         36,928\n",
       "│    └─ReLU: 2-4                         [1, 64, 224, 224]         --\n",
       "│    └─MaxPool2d: 2-5                    [1, 64, 112, 112]         --\n",
       "│    └─Conv2d: 2-6                       [1, 128, 112, 112]        73,856\n",
       "│    └─ReLU: 2-7                         [1, 128, 112, 112]        --\n",
       "│    └─Conv2d: 2-8                       [1, 128, 112, 112]        147,584\n",
       "│    └─ReLU: 2-9                         [1, 128, 112, 112]        --\n",
       "│    └─MaxPool2d: 2-10                   [1, 128, 56, 56]          --\n",
       "│    └─Conv2d: 2-11                      [1, 256, 56, 56]          295,168\n",
       "│    └─ReLU: 2-12                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-13                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-14                        [1, 256, 56, 56]          --\n",
       "│    └─Conv2d: 2-15                      [1, 256, 56, 56]          590,080\n",
       "│    └─ReLU: 2-16                        [1, 256, 56, 56]          --\n",
       "│    └─MaxPool2d: 2-17                   [1, 256, 28, 28]          --\n",
       "│    └─Conv2d: 2-18                      [1, 512, 28, 28]          1,180,160\n",
       "│    └─ReLU: 2-19                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-20                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-21                        [1, 512, 28, 28]          --\n",
       "│    └─Conv2d: 2-22                      [1, 512, 28, 28]          2,359,808\n",
       "│    └─ReLU: 2-23                        [1, 512, 28, 28]          --\n",
       "│    └─MaxPool2d: 2-24                   [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-25                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-26                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-27                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-28                        [1, 512, 14, 14]          --\n",
       "│    └─Conv2d: 2-29                      [1, 512, 14, 14]          2,359,808\n",
       "│    └─ReLU: 2-30                        [1, 512, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-31                   [1, 512, 7, 7]            --\n",
       "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
       "├─Sequential: 1-3                        [1, 4096]                 --\n",
       "│    └─Flatten: 2-32                     [1, 25088]                --\n",
       "│    └─Sequential: 2-33                  [1, 4096]                 --\n",
       "│    │    └─Linear: 3-1                  [1, 4096]                 102,764,544\n",
       "│    │    └─ReLU: 3-2                    [1, 4096]                 --\n",
       "│    │    └─Dropout: 3-3                 [1, 4096]                 --\n",
       "│    │    └─Linear: 3-4                  [1, 4096]                 16,781,312\n",
       "│    │    └─ReLU: 3-5                    [1, 4096]                 --\n",
       "==========================================================================================\n",
       "Total params: 134,260,544\n",
       "Trainable params: 134,260,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 15.48\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 108.45\n",
       "Params size (MB): 537.04\n",
       "Estimated Total Size (MB): 646.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%\n",
    "# from torchvision import summary\n",
    "# @ unit test : model versions eval ability\n",
    "summary(list_of_models[2][0], input_size=[1,3,224,224])\n",
    "# print(list_of_models[1][0])\n",
    "# print(list_of_models[0][0])\n",
    "# type(list_of_models[0][0])\n",
    "# type(list_of_models[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ testing : CNN model motification\n",
    "# from other_program.custom_model.VGG16 import VGG16\n",
    "# import inspect\n",
    "\n",
    "# model_0dot0 = VGG16()\n",
    "\n",
    "# list_of_dummy_model = createDetailLayerVersions(model_0dot0)\n",
    "\n",
    "# summary(list_of_dummy_model[1][0], [1,3,224,224])\n",
    "\n",
    "# for i in range(calDetailModelLayersNum(model_0dot0)):\n",
    "#     try : \n",
    "#         summary(list_of_dummy_model[i][0], input_size=[1,3,224,224],depth=4)\n",
    "#     except:\n",
    "#         print(\"mat error : {} layer\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 記錄下不同layer的輸出接到xgb的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 4]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 0\n",
      "======eval finish!=========\n",
      "best acc is : -0.720635\n",
      "best f1 is : -0.715561\n",
      "model_name : layer:44\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 46\n",
      "======eval finish!=========\n",
      "best acc is : -0.68254\n",
      "best f1 is : -0.672501\n",
      "model_name : layer:43\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 72\n",
      "======eval finish!=========\n",
      "best acc is : -0.657143\n",
      "best f1 is : -0.646655\n",
      "model_name : layer:42\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 14\n",
      "======eval finish!=========\n",
      "best acc is : -0.606349\n",
      "best f1 is : -0.588125\n",
      "model_name : layer:41\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 22\n",
      "======eval finish!=========\n",
      "best acc is : -0.650794\n",
      "best f1 is : -0.636268\n",
      "model_name : layer:40\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 22\n",
      "======eval finish!=========\n",
      "best acc is : -0.68254\n",
      "best f1 is : -0.670056\n",
      "model_name : layer:39\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 32\n",
      "======eval finish!=========\n",
      "best acc is : -0.704762\n",
      "best f1 is : -0.699137\n",
      "model_name : layer:38\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 65\n",
      "======eval finish!=========\n",
      "best acc is : -0.622222\n",
      "best f1 is : -0.611495\n",
      "model_name : layer:37\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 76\n",
      "======eval finish!=========\n",
      "best acc is : -0.590476\n",
      "best f1 is : -0.580254\n",
      "model_name : layer:36\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "Best iteration: 40\n",
      "======eval finish!=========\n",
      "best acc is : -0.603175\n",
      "best f1 is : -0.592223\n",
      "model_name : layer:35\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'XGB_model_results_vgg16.csv'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "XGB_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"iters\":[]}\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    # 獲取model的output size\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 224, 224]).to(device))\n",
    "        except RuntimeError:\n",
    "            print(\"mat mismatch!!\")\n",
    "            continue\n",
    "        \n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "    # TODO : if out of memory, escape \n",
    "    try:\n",
    "        f1, acc, iter = recordXGBoutput(model, train_dataloader, test_dataloader,enable_muti_module=False)\n",
    "    except MemoryError:\n",
    "        print(\"Not enough memory!! escape form loop\")\n",
    "        break\n",
    "    except RuntimeError:\n",
    "        print(\"mat mismatch!!\")\n",
    "        continue\n",
    "    print(\"best acc is : {}\".format(acc[iter]))\n",
    "    print(\"best f1 is : {}\".format(f1[iter]))\n",
    "    print(\"model_name : {}\".format(model_name) )\n",
    "    # cal output size\n",
    "    \n",
    "    XGB_res[\"model_name\"].append(model_name)\n",
    "    XGB_res[\"output_size\"].append(input_size)\n",
    "    XGB_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    XGB_res[\"acc\"].append(acc[iter])\n",
    "    XGB_res[\"f1\"].append(f1[iter])\n",
    "    XGB_res[\"iters\"].append(iter)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(XGB_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = \"XGB_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 紀錄其他ML模型的結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is :[1, 4]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 0.2426 秒\n",
      "預測時間 in 0.0103 秒\n",
      "預測時間 in 0.0067 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.6692 , 0.6762。\n",
      "======eval finish!=========\n",
      "layer:44\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 1.9638 秒\n",
      "預測時間 in 0.0310 秒\n",
      "預測時間 in 0.0150 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.6352 , 0.6476。\n",
      "======eval finish!=========\n",
      "layer:43\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 1.9601 秒\n",
      "預測時間 in 0.0210 秒\n",
      "預測時間 in 0.0150 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.6273 , 0.6444。\n",
      "======eval finish!=========\n",
      "layer:42\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 5.2605 秒\n",
      "預測時間 in 0.0180 秒\n",
      "預測時間 in 0.0110 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.6386 , 0.6508。\n",
      "======eval finish!=========\n",
      "layer:41\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 1.6654 秒\n",
      "預測時間 in 0.0280 秒\n",
      "預測時間 in 0.0180 秒\n",
      "訓練集的 F1 score和acc分別為: 1.0000 , 1.0000。\n",
      "測試集的 F1 score和acc分別為: 0.6496 , 0.6635。\n",
      "======eval finish!=========\n",
      "layer:40\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 1.8165 秒\n",
      "預測時間 in 0.0170 秒\n",
      "預測時間 in 0.0110 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.6519 , 0.6667。\n",
      "======eval finish!=========\n",
      "layer:39\n",
      "input size is :[1, 4096]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 4.6937 秒\n",
      "預測時間 in 0.0350 秒\n",
      "預測時間 in 0.0140 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.7010 , 0.7016。\n",
      "======eval finish!=========\n",
      "layer:38\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 4.4983 秒\n",
      "預測時間 in 0.0329 秒\n",
      "預測時間 in 0.0194 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.6270 , 0.6349。\n",
      "======eval finish!=========\n",
      "layer:37\n",
      "input size is :[1, 25088]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 4.4167 秒\n",
      "預測時間 in 0.0280 秒\n",
      "預測時間 in 0.0150 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.6358 , 0.6413。\n",
      "======eval finish!=========\n",
      "layer:36\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 4.5294 秒\n",
      "預測時間 in 0.0260 秒\n",
      "預測時間 in 0.0230 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.6043 , 0.6254。\n",
      "======eval finish!=========\n",
      "layer:35\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n",
      "訓練時間 4.4534 秒\n",
      "預測時間 in 0.0531 秒\n",
      "預測時間 in 0.0250 秒\n",
      "訓練集的 F1 score和acc分別為: 0.9984 , 0.9984。\n",
      "測試集的 F1 score和acc分別為: 0.5930 , 0.6254。\n",
      "======eval finish!=========\n",
      "layer:34\n",
      "input size is :[1, 512, 7, 7]\n",
      "cal CNN model output...\n",
      "feature size is :315\n",
      "feature size is :613\n",
      "訓練 RandomForestClassifier 模型，樣本數: 613。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from helperFunction.helperFunctions import flattenTensor\n",
    "# print(recordXGBoutput(model_0, train_dataloader, test_dataloader, \"resnet\", {}))\n",
    "\n",
    "# clf setting \n",
    "\n",
    "# csv_model_name , clf = \"Linear\" , LogisticRegression()\n",
    "# csv_model_name , clf = \"SVC\" , SVC(kernel='rbf',gamma='auto', probability=True)\n",
    "csv_model_name , clf = \"RandomForest\" , RandomForestClassifier(n_estimators=100)\n",
    "# csv_model_name , clf = \"XGB\" , xgb.XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "linear_res = {\"model_name\":[],\n",
    "       \"output_size\":[],\n",
    "       \"num_parm\":[],\n",
    "       \"acc\":[],\n",
    "       \"f1\":[],\n",
    "       \"eval_time\":[]}\n",
    "\n",
    "\n",
    "layer_cnt = 0\n",
    "\n",
    "for model, model_name in list_of_models:\n",
    "    input_size = \"\"\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            dummy_output = model(torch.rand([1, 3, 244, 244]).to(device))\n",
    "        except:\n",
    "            print(\"MAT ERROR!!!\")\n",
    "            continue\n",
    "        input_size = str(dummy_output.shape)[11:-1]\n",
    "\n",
    "    print(\"input size is :{}\".format(input_size))\n",
    "\n",
    "    f1, acc = recordMLoutput(clf, model, train_dataloader, test_dataloader,enable_muti_module=False)\n",
    "\n",
    "\n",
    "    # cal output size\n",
    "    \n",
    "    linear_res[\"model_name\"].append(model_name)\n",
    "    linear_res[\"output_size\"].append(input_size)\n",
    "    linear_res[\"num_parm\"].append(str(flattenTensor(dummy_output).shape)[11:-1])\n",
    "    linear_res[\"acc\"].append(acc)\n",
    "    linear_res[\"f1\"].append(f1)\n",
    "\n",
    "    layer_cnt += 1\n",
    "    if layer_cnt >= 15:\n",
    "        break\n",
    "\n",
    "    print(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# 將字典轉換為DataFrame，但這次是轉置後的形式\n",
    "df_transposed = pd.DataFrame.from_dict(linear_res, orient='index').transpose()\n",
    "\n",
    "# 將轉置後的DataFrame存儲為CSV檔案\n",
    "csv_file_path_transposed = csv_model_name + \"_model_results_\" + model_0_name + \".csv\"\n",
    "df_transposed.to_csv(csv_file_path_transposed, index=False)\n",
    "\n",
    "# 返回CSV檔案的儲存路徑\n",
    "csv_file_path_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'layer:62_model_results_resnet18.csv'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file_path_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 25088])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @ unit test\n",
    "from helperFunction.helperFunctions import flattenExceptDim0\n",
    "output = flattenExceptDim0(torch.rand([32,512,7,7]))\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate nn output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50251264,)\n",
      "(2003,)\n"
     ]
    }
   ],
   "source": [
    "print(test_features.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add features (LBP, csv...etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_features):\n\u001b[1;32m----> 2\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_feature_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_features):\n\u001b[0;32m      4\u001b[0m     feature \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((feature , np\u001b[38;5;241m.\u001b[39marray(train_feature_vectors[idx])))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 3 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "for idx,feature in enumerate(test_features):\n",
    "    feature = np.concatenate((feature , np.array(test_feature_vectors[idx])))\n",
    "for idx,feature in enumerate(train_features):\n",
    "    feature = np.concatenate((feature , np.array(train_feature_vectors[idx])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練 XGBClassifier 模型，樣本數: 201005056。\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please reshape the input data into 2-dimensional matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulti:softmax\u001b[39m\u001b[38;5;124m'\u001b[39m, num_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m f1, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtest_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# xgb_model.fit(features, labels)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\skin_cancer_dataset\\program\\helperFunction\\XgbHelperFunction.py:18\u001b[0m, in \u001b[0;36mtrain_predict\u001b[1;34m(clf, X_train, y_train, X_test, y_test, evalByMMSE)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練 \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m 模型，樣本數: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m。\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(clf\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_train)))\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 訓練模型\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 在訓練集上評估模型\u001b[39;00m\n\u001b[0;32m     20\u001b[0m res1 , res2 \u001b[38;5;241m=\u001b[39m predict_labels(clf, X_train, y_train, evalByMMSE)\n",
      "File \u001b[1;32mc:\\Users\\E\\Desktop\\skin_cancer_dataset\\program\\helperFunction\\XgbHelperFunction.py:65\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[1;34m(clf, X_train, y_train)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# 紀錄訓練時間\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# print(\"訓練資料 : \".format(X_train[0:4]))\u001b[39;00m\n\u001b[0;32m     64\u001b[0m start \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m---> 65\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m end \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m訓練時間 \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m 秒\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end \u001b[38;5;241m-\u001b[39m start))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1500\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1489\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[0;32m   1491\u001b[0m (\n\u001b[0;32m   1492\u001b[0m     model,\n\u001b[0;32m   1493\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1499\u001b[0m )\n\u001b[1;32m-> 1500\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1520\u001b[0m     params,\n\u001b[0;32m   1521\u001b[0m     train_dmatrix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m   1531\u001b[0m )\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:521\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[0;32m    502\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m    503\u001b[0m     X: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[0;32m    518\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[0;32m    519\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:958\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 958\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[0;32m    962\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1529\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m   1510\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1522\u001b[0m         )\n\u001b[0;32m   1523\u001b[0m     ):\n\u001b[0;32m   1524\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1525\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1526\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1527\u001b[0m         )\n\u001b[1;32m-> 1529\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:1588\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[1;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[0;32m   1576\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[0;32m   1577\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[0;32m   1578\u001b[0m )\n\u001b[0;32m   1579\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1581\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[0;32m   1587\u001b[0m )\n\u001b[1;32m-> 1588\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m _check_call(ret)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:576\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    575\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:557\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[1;34m(self, fn, dft_ret)\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:641\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n\u001b[0;32m    640\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1280\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1280\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:730\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    729\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 730\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\core.py:632\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[1;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n\u001b[1;32m--> 632\u001b[0m \u001b[43mdispatch_proxy_set_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allow_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mset_info(\n\u001b[0;32m    634\u001b[0m     feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[0;32m    635\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39mfeature_types,\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    637\u001b[0m )\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_ref \u001b[38;5;241m=\u001b[39m ref\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:1331\u001b[0m, in \u001b[0;36mdispatch_proxy_set_data\u001b[1;34m(proxy, data, cat_codes, allow_host)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Dispatch for QuantileDMatrix.\"\"\"\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_cudf_ser(data) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pandas_series(data):\n\u001b[1;32m-> 1331\u001b[0m     \u001b[43m_check_data_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_cudf_df(data):\n\u001b[0;32m   1334\u001b[0m     \u001b[38;5;66;03m# pylint: disable=W0212\u001b[39;00m\n\u001b[0;32m   1335\u001b[0m     proxy\u001b[38;5;241m.\u001b[39m_set_data_from_cuda_columnar(data, cast(List, cat_codes))\n",
      "File \u001b[1;32mc:\\Users\\E\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\data.py:57\u001b[0m, in \u001b[0;36m_check_data_shape\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_data_shape\u001b[39m(data: DataType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease reshape the input data into 2-dimensional matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Please reshape the input data into 2-dimensional matrix."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from helperFunction.XgbHelperFunction import train_predict\n",
    "\n",
    "# 創建XGBoost分類器\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=7)\n",
    "\n",
    "# 訓練模型\n",
    "f1, acc = train_predict(xgb_model, train_features, train_labels,  test_features, test_labels)\n",
    "# xgb_model.fit(features, labels)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more detail XGB info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.23198\ttrain-Accuracy:0.87132\ttrain-F1-score:0.86772\tval-mlogloss:1.25322\tval-Accuracy:0.83974\tval-F1-score:0.83289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-mlogloss:0.94329\ttrain-Accuracy:0.87881\ttrain-F1-score:0.87539\tval-mlogloss:0.98333\tval-Accuracy:0.84373\tval-F1-score:0.83783\n",
      "[2]\ttrain-mlogloss:0.76241\ttrain-Accuracy:0.88118\ttrain-F1-score:0.87782\tval-mlogloss:0.81834\tval-Accuracy:0.84473\tval-F1-score:0.83857\n",
      "[3]\ttrain-mlogloss:0.63912\ttrain-Accuracy:0.88280\ttrain-F1-score:0.87919\tval-mlogloss:0.70796\tval-Accuracy:0.84323\tval-F1-score:0.83756\n",
      "[4]\ttrain-mlogloss:0.54987\ttrain-Accuracy:0.88530\ttrain-F1-score:0.88217\tval-mlogloss:0.62823\tval-Accuracy:0.84523\tval-F1-score:0.83996\n",
      "[5]\ttrain-mlogloss:0.48412\ttrain-Accuracy:0.88892\ttrain-F1-score:0.88588\tval-mlogloss:0.57188\tval-Accuracy:0.84523\tval-F1-score:0.83965\n",
      "[6]\ttrain-mlogloss:0.43472\ttrain-Accuracy:0.88954\ttrain-F1-score:0.88653\tval-mlogloss:0.53085\tval-Accuracy:0.84623\tval-F1-score:0.84082\n",
      "[7]\ttrain-mlogloss:0.39629\ttrain-Accuracy:0.89179\ttrain-F1-score:0.88892\tval-mlogloss:0.50091\tval-Accuracy:0.84573\tval-F1-score:0.84014\n",
      "[8]\ttrain-mlogloss:0.36640\ttrain-Accuracy:0.89328\ttrain-F1-score:0.89043\tval-mlogloss:0.47813\tval-Accuracy:0.84972\tval-F1-score:0.84447\n",
      "[9]\ttrain-mlogloss:0.34207\ttrain-Accuracy:0.89578\ttrain-F1-score:0.89311\tval-mlogloss:0.46274\tval-Accuracy:0.84723\tval-F1-score:0.84180\n",
      "Best iteration: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, log_loss\n",
    "\n",
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "\n",
    "    # 将预测结果转换为类别标签\n",
    "    mlogloss = log_loss(labels, preds, labels=np.unique(dtrain.get_label()))\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # 用 'weighted' 适应类别不平衡\n",
    "    return [('Accuracy', acc), ('F1-score', f1), ('mlogloss', mlogloss)]\n",
    "\n",
    "dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "dval = xgb.DMatrix(test_features, label=test_labels)\n",
    "\n",
    "# 设置参数，注意要更改 'objective' 为多分类的目标函数\n",
    "num_class = len(np.unique(train_labels))  # 获取类别数\n",
    "params = {'objective': 'multi:softprob','eval_metric': 'mlogloss', 'num_class': num_class}\n",
    "\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "bst = xgb.train(params, dtrain, num_boost_round=105, \n",
    "                evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "                custom_metric=custom_eval, evals_result=evals_result, \n",
    "                early_stopping_rounds=10,\n",
    "                verbose_eval=True)\n",
    "\n",
    "print(f\"Best iteration: {bst.best_iteration}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
